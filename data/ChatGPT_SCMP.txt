The growing popularity of artificial intelligence (AI) is expected to lift sales of personal computers (PCs) in the coming years, as hardware firms rush to embed the technology into devices, according to recent research by IT consultancy IDC.
Shipments of PCs have slowed in the past two years after a wave of pandemic-induced purchases from consumers working from home in 2020 and 2021. The Asia-Pacific PC market – which includes desktop and laptop computers and workstations – is estimated to fall 7.6 per cent this year after a 11.6 per cent slump in 2022.
However, manufacturers are counting on the rising adoption of generative AI products, which include the likes of Microsoft-backed OpenAI’s ChatGPT and Chinese internet search giant Baidu’s Ernie Bot, to spur consumers to upgrade to devices with built-in AI capabilities, driving up demand in an otherwise sluggish PC market, according to IDC.
OpenAI announced this week at its first-ever developer conference that ChatGPT, which was launched just a year ago, now has over 100 million weekly active users globally.
How does China’s AI stack up against ChatGPT?
How does China’s AI stack up against ChatGPT?
Baidu’s Ernie Bot, launched in March, had amassed over 70 million users in China, said Wang Haifeng, the company’s technology chief, on Thursday at the annual World Internet Conference that took place in the picturesque canal town of Wuzhen, in eastern Zhejiang province.
ChatGPT is officially unavailable in China.
Users have been eagerly experimenting with the latest generation of AI chatbots, which are capable of handling tasks such as summarising long text, drafting emails and work reports, transcribing and translating recordings in various languages, among other functions.
PC makers are working to integrate those capabilities with their products.
Lenovo aims to turn “personal computers” into “personalised computers” with AI technology, vice-president Ablikim Ablimit said at an industry event in Beijing on Thursday.
Your weekly round-up of the biggest trending tech stories from China.
An AI-powered laptop would be able to create work reports “based on [its learning of] your preference, your historic documents”, Ablimit told an audience at an event held in Beijing on Thursday. But reaching that goal would require the concerted effort of the whole PC and AI sectors, he added.
AI PCs, defined as computers equipped with chips that have built-in AI capabilities, are forecast to account for 86 per cent of the estimated 34 million laptops to be shipped in China in 2027, according to IDC.
Examples of AI-enabled processors include Advanced Micro Devices’ Ryzen Pro 7040 series launched earlier this year, which was described as one of the world’s first x86 processors with an integrated AI engine.
Smartphone makers are also trying to bring AI capabilities to handsets.
China’s Oppo, the world’s fourth-largest smartphone vendor according to recent data from research firm Canalys, “is very optimistic about the future integration of AI and large-language-model applications with smartphones”, head of Oppo Research Institute Jason Liao told reporters last month.
The company in August unveiled its proprietary large language model AndesGPT for testing, with the aim of upgrading its voice assistant Xiaobu to improve interactions with users.
Sister brand Vivo also said last month it would add its own AI model into its new operating system OriginOS 4.
An AI model under attack could mistake giant pandas for humans or fail to detect harmful content, according to a research team in Beijing that says it discovered an effective method for attacking ChatGPT and other popular commercial AI models.
Doctored images used by the researchers appeared almost identical to the original, but they could effectively circumvent the models’ mechanisms designed to filter out toxic information.
The findings highlight significant security concerns within artificial intelligence and help shed light on the vulnerabilities of commercial multimodal large language models (MLLMs) from tech giants including Google, Microsoft and Baidu.
At the inaugural Global AI Security Summit held in the UK last week, representatives from the US, Britain, the European Union, China and India signed the Bletchley Declaration, an unprecedented deal to encourage safe and ethical development and use of AI.
Wu Zhaohui, China’s vice-minister of science and technology, took part in the conference and presented proposals, advocating for stronger technical risk controls in AI governance.
Zhu Jun and Su Hang from Tsinghua University’s department of computer science and technology said in a paper that criminals could exploit these inherent AI vulnerabilities to produce harmful content. The paper did not detail how those weaknesses could be exploited.
“As large-scale foundation models such as ChatGPT and Bard are increasingly utilised for various tasks, their security issues become a pressing concern for the public,” they stated in their paper published on the arXiv website in October.
MLLMs such as ChatGPT or Google’s Bard typically process image content into text in two steps: they first use a visual encoder to extract features from the image; and then feed these features into the model to generate a corresponding text description.
The research team outlined two types of adversarial attacks against MLLMs: image feature attack and text description attack.
Your daily must-read of essential stories from China, including politics, economy and current affairs.
The former changes the features of a sample, affecting subsequent judgments. The latter attacks the entire process, resulting in generated descriptions that differ from the correct ones, thereby confusing the machine.
The adversarial attacks made minute, almost imperceptible, changes to the original picture. To the human eye, the altered samples showed almost no difference to the original image.
Bard – with its face detection and toxicity detection mechanisms – actively rejects images containing faces or violent, bloody, or pornographic content to protect privacy and prevent abuse.
The overall decision-making process of MLLMs is a “black box”, with its architecture and parameters remaining unknown. However, the algorithms for face and toxicity detection are familiar to scientists with researchers launching attacks on these sub-models.
The team applied similar mathematical manipulations and manually collected photos to attack Bard. According to their paper, 38 per cent of images bypassed the face detector, and 36 per cent evaded the toxicity detector.
For instance, in a breach of its own privacy protections, Bard identified a Korean singer and provided detailed content descriptions for an image of a group of soldiers holding guns, which contained violent information.
The experiment emphasises the potential for malicious attackers to use Bard to generate inappropriate descriptions of harmful content.
With a similar attack method, a giant panda’s face is misclassified as a woman’s face by Bard, a group of antelopes is misclassified as hands by GPT-4V and a bald eagle is misclassified as a cat and a dog in Bing Chat, a cup of coffee is misclassified as a watch in Baidu’s Ernie Bot, whose description was in Chinese.
These findings suggest that most MLLMs have vulnerabilities in their image content recognition.
From the code the Chinese team provided with the paper, 200 generated adversarial examples can mislead AI models to output the wrong image descriptions, with a 22 per cent success rate against Bard, 26 per cent success against Bing Chat and an 86 per cent attack success rate against Ernie Bot.
“The current defence mechanisms of Bard can be easily bypassed by adversarial examples, highlighting the need for targeted defences to ensure the safety of MLLMs,” the team said in the paper.
How does China’s AI stack up against ChatGPT?
How does China’s AI stack up against ChatGPT?
However, there is an imbalance between research into attacking and defending AI models. An anonymous researcher said most annual publications on adversarial models focused on attacks, with only a few investigating defence.
“This is because a single defence strategy may only be effective against one type of attack, and it is far more difficult to defend against all potential attacks than to target a fixed objective,” he said.
“Traditional defence methods that increase robustness might lead to a trade-off in accuracy and can be computationally expensive, making them challenging to apply to large models,” Zhu stated in the paper.
He suggested that preprocessing-based defences might be more suitable for large-scale foundation models.
Despite extensive research, defending against adversarial attacks on vision models remains an unresolved issue.
For every five new jobs in artificial intelligence (AI) in China, there are only two qualified workers in the labour market, a sign of the serious shortage of talent in the hot sector, according to a newly published report.
The surging demand is largely driven by increasing competition among Chinese Big Tech firms, including TikTok parent ByteDance, e-commerce powerhouse Alibaba Group Holding, video gaming giant Tencent Holdings and telecommunications equipment maker Huawei Technologies, to launch their large language models (LLMs) and AI applications, according to a report by Maimai, a career-focused social network service.
ByteDance had the largest demand among its peers, with the most openings for AI jobseekers over the last three years.
OpenAI unveiled a marketplace on Monday that enables users to access personalised artificial intelligence “apps” for tasks like teaching maths or designing stickers, signalling an ambition to expand its consumer business.
OpenAI CEO Sam Altman shared the updates at the AI lab’s first developer conference, which attracted 900 developers from around the world and marked the company’s latest attempt to capitalise on the popularity of ChatGPT by offering incentives to build in its ecosystem.
ChatGPT, launched in November 2022, now has 100 million weekly active users, Altman said.
When novelist Douglas Preston first started messing around with ChatGPT, he gave the AI software a challenge: could it write an original poem based on a character from some of his books?
“It came out with this terrific poem written in iambic pentameter,” Preston recalls. The result was impressive – and concerning.
“What really surprised me was how much it knew about this character; way more than it possibly could have gleaned from the internet,” Preston says.
The adventure writer suspected that the chatbot had somehow absorbed his work, presumably as part of the training process by which an artificial intelligence model ingests lots of data that it then synthesises into seemingly original content.
Those worries led Preston to sign on to a proposed class action lawsuit accusing OpenAI, the developer behind ChatGPT and a major player in the growing AI industry, of copyright infringement.
Preston is joined in the suit by a host of other big-name authors, including John Grisham, Jonathan Franzen, Jodi Picoult and George R.R. Martin – the notoriously slow-to-publish Game of Thrones author who, Preston says, joined out of frustration that fans were using ChatGPT to preemptively generate the last book in his series.
OpenAI, for its part, has contended that training an AI system falls under fair use protections, especially given the extent to which AI transforms the underlying training data into something new.
A spokesman for OpenAI said the firm respects authors’ rights and believes they should “benefit from AI technology”.
“We’re having productive conversations with many creators around the world, including the Authors Guild, and have been working cooperatively to understand and discuss their concerns about AI,” the spokesman said of America’s oldest and largest organisation for published writers.
Nevertheless, the publishing industry is pushing back as it reckons with a software boom that’s given anyone with Wi-fi the power to automatically generate large reams of text.
In addition to Preston’s suit, various other groups of authors are pursuing their own proposed class action suits against OpenAI.
“Everybody’s realising to what extent their data, their information, their creativity, has been absorbed,” says Ed Nawotka, an editor at American trade news publication Publishers Weekly. There is, in the industry, a degree of “abject panic”, he says.
In one recent pair of lawsuits, American comedian and actress Sarah Silverman accused OpenAI as well as Meta – Facebook’s parent company and a major AI developer itself – of copyright infringement. The two companies have since pushed to get most of Silverman’s cases dismissed.
A different suit recently found Paul Tremblay (The Cabin at the End of the World) and Mona Awad (Bunny) suing OpenAI for copyright violations – the company is trying to get that one mostly dismissed too – while Michael Chabon (The Yiddish Policemen’s Union) is a plaintiff in two additional legal actions that are targeting OpenAI and Meta, respectively.
And in July, the Authors Guild – a professional trade group, not a labour union – sent several technology companies an open letter calling for consent, credit and fair compensation when writers’ works are used to train AI models.
Among the signatories were Margaret Atwood, Dan Brown, James Patterson, Suzanne Collins, Roxane Gay and Celeste Ng.
That’s all on top of the nearly five-month-long strike that Hollywood screenwriters recently undertook that led to, among other things, new regulations on the use of AI for script generation. A separate strike, still ongoing, has found screen actors rallying around AI concerns of their own.
The lawsuit in which Preston is involved, which features 17 other named plaintiffs including the Authors Guild, claims that OpenAI copied the authors’ works “without permission or consideration” to train AI programs that now compete with those authors for readers’ time and money.
The suit also takes issue with ChatGPT’s generation of derivative works, or “material that is based on, mimics, summarises, or paraphrases [the] Plaintiffs’ works, and harms the market for them”.
The plaintiffs are seeking damages for their lost licensing opportunities and “market usurpation”, as well as an injunction against future such practices, on behalf of American fiction authors whose copyrighted works were used to train OpenAI software.
Since the plaintiffs’ books aren’t freely available on the open web, he adds, OpenAI “almost certainly” accessed them via alleged piracy sites such as the file-sharing platform LibGen.
OpenAI declines to answer a question about whether the plaintiffs’ books were part of ChatGPT’s training data or accessed via file-sharing sites such as LibGen. In a statement to the US Patent and Trademark Office cited in the Authors Guild suit, OpenAI stated that modern AI systems are sometimes trained on publicly available data sets that include copyrighted works.
Connelly never got to decide whether his books would be used to train an AI, he said, but if he’d been asked – even if there were money on the table – he would probably have opted out.
The idea of ChatGPT writing an unofficial Bosch sequel strikes him as a violation; even when Amazon adapted the series into a TV show, he says, he had some control over the scripts and casting.
But whether the law will allow the machines to do so is a different question.
The various lawsuits against OpenAI allege copyright violations. But copyright law – and especially fair use, the area of law governing when copyrighted work can be incorporated into other endeavours, such as for the sake of education or criticism – still doesn’t offer a cut-and-dried answer to how these lawsuits will shake out.
“We’ve got kind of a push and pull right now in the case law,” says intellectual property lawyer Lance Koonce, a partner at the law firm Klaris, pointing to two recent US Supreme Court cases that offer competing models of fair use.
In one, Authors Guild vs. Google, the court held that Google was allowed to digitise millions of copyrighted books to make them searchable.
In the other, Andy Warhol Foundation for the Visual Arts Inc. vs. Goldsmith, the court found that the titular pop artist’s incorporation of a photographer’s work into his own art didn’t fall under fair use because Warhol’s art was commercial and had the same basic purpose as the original photo.
“These AI cases – and especially the Authors Guild case [against OpenAI] – fall into that tension,” Koonce said.
In its patent office statement, OpenAI argued that training artificial intelligence software on copyrighted works “should not, by itself, harm the market for or value of copyrighted works” because the works are being consumed by software rather than real people.
Outside of legal avenues, stakeholders are already pitching solutions to this tension.
Suman Kanuganti, the chief executive of AI messaging platform Personal.ai, says the tech industry will probably adopt some sort of attribution standard that allows people who contribute to an AI’s training data to be identified and compensated.
Preston, the adventure novelist, agrees that there may yet be a path forward.
Licensing books to software developers through a centralised clearing house could provide authors with a new income stream while also securing high-quality training data for AI companies, he says, adding that the Authors Guild tried to set up such an arrangement with OpenAI at one point but that the two sides were unable to reach an agreement.
Chinese social networking and video gaming giant Tencent Holdings has baked its ChatGPT-like generative artificial intelligence (AI) model into scores of products and services as competition in the red-hot sector continues apace in the world’s second largest economy.
The Shenzhen-based firm’s Hunyuan large language model (LLM), the technology that underpins OpenAI’s ChatGPT and similar products, is now integrated into more than 180 services, it announced in a post published to the official Hunyuan account on Tencent’s WeChat.
The sprawling Chinese tech giant said it has seen improvements in many of its services that include the conferencing app Tencent Meeting and web-based word processor Tencent Docs, along with its online advertising business and WeChat search.
Chinese artificial intelligence (AI) company iFlytek, one of the first major tech firms in the country to launch an alternative to US chatbot ChatGPT, said its large language model (LLM) has now outperformed OpenAI’s GPT-3.5 within a Chinese language context.
iFlytek Spark 3.0, first unveiled by the company in May and released to the general public in September after winning regulatory approval, has outperformed GPT-3.5 in six abilities including text generation, logic reasoning, maths and coding, and is set to rival GPT-4 by the first half of 2024, according to iFlytek chairman Liu Qingfeng.
iFlytek’s LLM, the fourth version since an initial release, also scores roughly the same as GPT-3.5 in performing 48 tasks within an English language context, Liu said at a company event on Tuesday in Hefei, the company’s home city in eastern Anhui province.
Oppo is doubling down on its artificial intelligence (AI) research efforts as competition among smartphone makers to integrate ChatGPT-like products within handsets heats up.
The Chinese smartphone maker is “actively investing in generative AI”, including the development of its own large language model (LLM) named AndesGPT. Oppo’s research arm and engineering team for hardware and software are fully engaged in these efforts, said Jason Liao, president of Oppo Research Institute.
“We are very optimistic about the future integration of AI and LLM applications with smartphones,” Liao said in a media briefing on its start-up competition Inspiration Challenge in Singapore on Wednesday.
Big data, data science and artificial intelligence (AI) technologies like ChatGPT are shaping the finance industry, and asset manager BlackRock is bullish about using these fast-evolving technologies to help clients build better portfolios.
An AI model using machine learning can crunch massive amounts of data to help investment analysts assess economic conditions, said Jeff Shen, co-CIO and co-head of systematic active equity at BlackRock, the world’s biggest money manager with US$9.42 trillion in assets under management as of June 30.
Daily news, broker reports, expert insights and government statistics all provide data that can be analysed by AI to give clues about the underlying economic situation, he said. Meanwhile, geospatial information can be used to identify trends by analysing patterns in, for example, the movement of trucks in and out of a company’s warehouses, or foot traffic in a city.
OpenAI plans to introduce major updates for developers next month to make it cheaper and faster to build software applications based on its artificial intelligence (AI) models, as the ChatGPT maker tries to court more companies to use its technology, sources briefed on the plans told Reuters.
The updates include the addition of memory storage to its developer tools for using AI models. This could theoretically slash costs for application makers by as much as 20-times, addressing a major concern for partners whose cost of using OpenAI’s powerful models could pile up quickly, as they try to build sustainable businesses by developing and selling AI software.
The company also plans to unveil new tools such as vision capabilities that will enable developers to build applications with the ability to analyse images and describe them, with potential use cases in fields from entertainment to medicine.
Feel strongly about these letters, or any other aspects of the news? Share your views by emailing us your Letter to the Editor at letters@scmp.com or filling in this Google form. Submissions should not exceed 400 words, and must include your full name and address, plus a phone number for verification.
As the chief executive is scheduled to deliver his policy address later this month, I am writing on behalf of a coalition of the Hong Kong Game Industry Association, Hong Kong Digital Entertainment Association and Hong Kong Comics and Animation Federation. Our coalition’s vision is to revitalise local talent in the digital media industry, a pivotal sector of the Hong Kong digital economy.
Hong Kong, which has been referred to as the “Hollywood of the East”, boasts a rich history as a centre for cultural and artistic exchange. We firmly believe that it is imperative to build upon this heritage by fully embracing the art tech movement, in line with the nationwide push for the development of the digital economy.
While the Hong Kong government has implemented various programmes to support the arts, we contend that the city’s current local talent pool is insufficient in both quantity and quality to meet the challenges that lie ahead.
In light of increasing rote-task automation and rapid advancements in artificial intelligence technologies, we recommend three initiatives to facilitate the transition of Hong Kong’s workforce to high-value occupations, thus bolstering the talent pool for our industries.
First, upskill the existing workforce. This can be achieved by expanding retraining programmes, particularly those administered by the Employees Retraining Board (ERB). Recent research highlights the significant economic impact of employees with digital skills.
Second, extend the “Love Upgrading Special Scheme”, which when it was launched during the Covid-19 pandemic was open to degree holders. Degree holders should have access to ERB’s IT-related courses, as possessing a degree no longer guarantees secure employment or current market-relevant skills. Future iterations of this scheme could focus on innovative tech, such as Web3 and art tech, to address talent shortages in future-proof industries, including digital media.
Third, redesign the ERB subsidy programme. To ensure committed talent is directed towards industry roles, we suggest shifting the subsidy focus from students to employers. This approach is more likely to achieve the board’s goal of facilitating employment.
Our weekly round-up of the best news, stories and opinion from Hong Kong.
In conclusion, all stakeholders in the digital economy, including the government, employers and employees, have a role to play in upskilling and retraining the current workforce. By providing opportunities for skill acquisition and collaborating effectively, we can prepare our workforce for the evolving job market. This will not only enhance individual careers but also contribute to Hong Kong’s overall economic growth.
Investing in equipping our people with the skills demanded by the digital economy will strengthen Hong Kong’s competitiveness on a global scale.
It has been about a month since the start of the new academic year. While students are grappling with new knowledge, they are also dealing with new software pervasive in academic life – generative artificial intelligence (GenAI). While each university and school has its own approach to the use of GenAI, in general, there are three areas in which students need to be cautious while using it in assessments.
First and foremost, GenAI may provide fake or inaccurate information. Without scrutiny, students could easily include incorrect material in their assessments. This will not just adversely affect the quality of the assignment completed, but also the quality of learning.
GenAI also sometimes generates false information to meet a prompt’s requirements. For example, GenAI may cite a completely made-up source in its answer. Therefore, students must cross-check the accuracy of the information GenAI provides.
Second, GenAI can lead to plagiarism. The information it provides could be a direct copy without proper acknowledgement of the original source.
If plagiarism detection software discovers this in a submitted assignment, students, rather than GenAI, would be held responsible. Students should check the original source and make sure ideas are appropriately acknowledged.
Finally, GenAI cannot provide in-depth critical insight. GenAI can be applied to homework that involves summarising. But students may still need to write argumentative essays themselves.
If students rely on GenAI to create the entire essay from scratch, the submission won’t score well or may even be given a failing grade. Hence, students shouldn’t rely solely on GenAI.
After all, human brains cannot be replaced by computers. We still need to perform higher-order thinking on our own. If not, what’s the meaning of our existence?
I had the privilege after 2009 to work briefly for both the Agriculture, Fisheries and Conservation Department and City University of Hong Kong, giving me insights into their projects. With regard to the recent double accreditation of the university’s veterinary school, I would like to inform readers that without the vision and dedication of Dr Howard Wong, the veterinary school would not have been established.
Pathfinders are often misunderstood. They need to have the good fortune to receive the gift of highly skilled and excellent professionals to follow their lead. And this has occurred here. But credit where credit is due, and unlike many pathfinders, Dr Wong is really easy to get along with.
OpenAI, the company behind ChatGPT, is exploring making its own artificial intelligence chips and has gone as far as evaluating a potential acquisition target, according to people familiar with the company’s plans.
The company has not yet decided to move ahead, according to recent internal discussions described to Reuters. However, since at least last year it discussed various options to solve the shortage of expensive AI chips that OpenAI relies on, according to people familiar with the matter.
These options have included building its own AI chip, working more closely with other chip makers including Nvidia and also diversifying its suppliers beyond Nvidia.
Artificial intelligence (AI) start-up OpenAI is rolling out a feature for its ChatGPT app that lets the chatbot respond to spoken questions and commands with speech of its own.
Starting over the next two weeks, users will be able to choose a voice in the chatbot app, picking from five personas with names like “Juniper,” “Breeze” and “Ember”. ChatGPT will then produce audio of the text it generates in that voice – for example, reading an AI-generated bedtime story out loud.
The feature will be available to people who subscribe to OpenAI’s US$20-per-month ChatGPT Plus service and enterprise users.
A founding investor in Cambricon Technologies, one of China’s leading artificial intelligence (AI) chip firms, has dumped nearly all of its equity stake in the Shanghai-listed company as losses mount amid economic headwinds and an industry downturn.
SDIC Venture Capital Management Co sold 7.4 million shares, or 99.98 per cent of its total stake, in Cambricon for a total of 1.48 billion yuan (US$203 million) between March to June this year, according to a Shanghai Stock Exchange filing by the company dated September 23.
SDIC Venture Capital, the venture capital arm established in 2016 by State Development and Investment Corporation, the largest state-owned investment holding company in China, manages a fund of around 30 billion yuan, according to its website.
A trade group for US authors has sued OpenAI in Manhattan federal court on behalf of prominent writers including John Grisham, Jonathan Franzen, George Saunders, Jodi Picoult and Game of Thrones novelist George R.R. Martin, accusing the company of unlawfully training its popular artificial-intelligence based chatbot ChatGPT on their work.
The proposed class-action lawsuit filed late on Tuesday by the Authors Guild joins several others from writers, source-code owners and visual artists against generative AI providers.
In addition to Microsoft-backed OpenAI, similar lawsuits are pending against Meta Platforms and Stability AI over the data used to train their AI systems.
Alibaba Group Holding’s cloud computing unit has opened its large language model (LLM) Tongyi Qianwen to the public, following a slew of similar moves by other local technology companies that received the government’s approval to launch commercial ChatGPT-like services.
In an article published on WeChat on Wednesday, Alibaba Group Intelligence Group said it aimed to “let every ordinary person and enterprise benefit from LLMs”, the technology that underpins cutting-edge artificial intelligence (AI) chatbots like OpenAI’s ChatGPT.
Alibaba Cloud started a beta test of Tongyi Qianwen in April. It has since worked with other Alibaba units, such as e-commerce platform Taobao and work communications tool DingTalk, as well as outside companies like smartphone brand Oppo, to train their own LLMs or develop applications based on Tongyi Qianwen, Alibaba Cloud said.
Chinese social media and video gaming giant Tencent Holdings will soon launch its artificial intelligence (AI) foundation model Hunyuan, which is expected at an event scheduled for Thursday, as the Chinese tech giant jostles for supremacy in the crowded domestic market for large language models (LLMs).
The company’s annual Global Digital Ecosystem Summit takes place on Thursday and Friday in Shenzhen, where there will be a speech about the Hunyuan model, according to the agenda.
The new model, which was under internal testing for a few months, will be “formally released” soon, Li Qiang, a vice-president at Tencent who heads its Government and Enterprise Business division, said at an industry conference held in the southwestern city of Chongqing on Monday, according to a report from Chinese media The Beijing News.
Baidu, the Chinese internet search giant that opened its chatbot to the general public last week, said it would soon launch the latest version of its large language model (LLM), casting a fresh vote of confidence in the nation’s artificial intelligence (AI) sector after Beijing granted its first batch of approvals for ChatGPT-style services.
Robin Li Yanhong, founder and CEO of Baidu, announced the imminent arrival of the Ernie 4 model at a conference held by the company’s cloud computing unit in Beijing on Tuesday, calling it “one of the best LLMs”.
Ernie 4 can be used to build AI applications for use across a wide range of traditional industries and business scenarios to improve efficiency, Li said.
Ashley Lam Cheuk-yiu, a second-year marketing student at the Hong Kong University of Science and Technology (HKUST), had an assignment to design an advertising campaign for a brand using concepts taught in class.
The 20-year-old wrote a 600-word essay that proposed tech giant Apple should use its famous “Think Different” slogan to promote human rights and gender equality as well as its iPhone products.
Then she asked ChatGPT, a generative artificial intelligence (AI) tool that emerged last year, to do the same assignment.
Baidu’s ChatGPT-like service Ernie Bot saw rapid adoption on the first day of its public roll-out, with the mobile app topping downloads on multiple sites, including Apple’s China iOS store, as millions of users tested the service with a wide range of questions, including some that it had trouble answering.
The Chinese government has lifted its tight lid on the country’s aspirational ChatGPT-wannabes, approving the first batch of generative artificial intelligence (AI) services for public release on the last day in August, two weeks after it enacted sweeping regulations on the technology.
The approved services include Ernie Bot, as well as those from AI specialist SenseTime, Sogou founder Wang Xiaochuan’s new venture Baichuan and state-backed Zhipu AI, among others.