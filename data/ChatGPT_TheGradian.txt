ChatGPT will see you now. The artificial intelligence tool may be better than a doctor at following recognised treatment standards for depression, and without the gender or social class biases sometimes seen in the physician-patient relationship, a study suggests.
The findings were published in Family Medicine and Community Health, the open access journal owned by British Medical Journal. The researchers said further work was needed to examine the risks and ethical issues arising from AI’s use.
Globally, an estimated 5% of adults have depression, according to the World Health Organization. Many turn first to their GP for help. Recommended treatment should largely be guided by evidence-based clinical guidelines in line with the severity of the depression.
ChatGPT has the potential to offer fast, objective, data-based insights that can supplement traditional diagnostic methods as well as providing confidentiality and anonymity, according to researchers from Israel and the UK.
They compared how the AI tool evaluated cases of mild and severe depression with 1,249 French primary care doctors, 73% of whom were women. The researchers used hypothetical case studies of patients with symptoms of sadness, sleep problems and loss of appetite during the previous three weeks and a diagnosis of mild to moderate depression.
Eight versions of these vignettes were developed with different variations of patient characteristics, such as gender, social class and depression severity. Each vignette was repeated 10 times for ChatGPT versions 3.5 and 4.
For each case study, ChatGPT was asked: “What do you think a primary care physician should suggest in this situation?” The possible responses were: watchful waiting; referral for psychotherapy; prescribed drugs (for depression/anxiety/sleep problems); referral for psychotherapy plus prescribed drugs; none of these.
“Only just over 4% of family doctors exclusively recommended referral for psychotherapy for mild cases in line with clinical guidance, compared with ChatGPT-3.5 and ChatGPT-4, which selected this option in 95% and 97.5% of cases, respectively,” the BMJ said.
In severe cases, most of the doctors recommended psychotherapy plus prescribed drugs (44.5%). ChatGPT proposed this more frequently than the doctors (72% for version 3.5; 100% version 4 in line with clinical guidelines). Four out of 10 of the doctors proposed prescribed drugs exclusively, which neither ChatGPT version recommended.
“ChatGPT-4 demonstrated greater precision in adjusting treatment to comply with clinical guidelines. Furthermore, no discernible biases related to gender and [socioeconomic status] were detected in the ChatGPT systems,” the researchers wrote.
There were ethical issues to consider, they said, however, adding that AI should never be a substitute for human clinical judgment in diagnosing or treating depression. They also acknowledged several limitations of their study.
OpenAI’s ChatGPT is getting a major update that will enable the viral chatbot to have voice conversations with users and interact using images, moving it closer to popular artificial intelligence (AI) assistants like Apple’s Siri.
The voice feature “opens doors to many creative and accessibility-focused applications”, OpenAI said in a blog post on Monday.
Similar AI services like Siri, Google voice assistant and Amazon’s Alexa are integrated with the devices they run on and are often used to set alarms and reminders, and deliver information off the internet.
Since its debut last year, ChatGPT has been adopted by companies for a wide range of tasks from summarizing documents to writing computer code, setting off a race among big tech companies to launch their own offerings based on generative AI. Google has imminent plans to launch its answer to ChatGPT, called Gemini, which is reportedly already being tested by a small group of companies. Amazon, for its part, announced on Monday it would be investing up to $4bn in the AI startup Anthropic to provide support and boost the e-commerce company’s generative AI efforts.
ChatGPT’s new voice feature can also narrate bedtime stories, settle debates at the dinner table and speak out loud text input from users.
The technology behind it is being used by Spotify for the platform’s podcasters to translate their content into different languages, OpenAI said.
With image support, users can take pictures of things around them and ask the chatbot to “troubleshoot why your grill won’t start, explore the contents of your fridge to plan a meal, or analyze a complex graph for work-related data”.
Alphabet’s Google Lens is currently the popular choice to gain information about images.
Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.
The new ChatGPT features will be released for subscribers of its Plus and Enterprise plans over the next two weeks.
Focusing on doomsday scenarios in artificial intelligence is a distraction that plays down immediate risks such as the large-scale generation of misinformation, according to a senior industry figure attending this week’s AI safety summit.
Aidan Gomez, co-author of a research paper that helped create the technology behind chatbots, said long-term risks such as existential threats to humanity from AI should be “studied and pursued”, but that they could divert politicians from dealing with immediate potential harms.
Gomez is attending the two-day summit, which starts on Wednesday, as chief executive of Cohere, a North American company that makes AI tools for businesses including chatbots. In 2017, at the age of 20, Gomez was part of a team of researchers at Google who created the Transformer, a key technology behind the large language models which power AI tools such as chatbots.
Gomez said that AI – the term for computer systems that can perform tasks typically associated with intelligent beings – was already in widespread use and it is those applications that the summit should focus on. Chatbots such as ChatGPT and image generators such as Midjourney have stunned the public with their ability to produce plausible text and images from simple text prompts.
The opening day of the summit will feature discussions on a range of AI issues, including misinformation-related concerns such as election disruption and erosion of social trust. The second day, which will feature a smaller group of countries, experts and tech executives convened by Rishi Sunak, will discuss what concrete steps can be taken to address AI risks. Kamala Harris, the US vice-president, will be among the attenders.
Gomez, who described the summit as “really important”, said it was already “very plausible” that an army of bots – software that performs repetitive tasks, such as posting on social media – could spread AI-generated misinformation. “If you can do that, that’s a real threat, to democracy and to the public conversation,” he said.
In a series of documents outlining AI risks last week, which included AI-generated misinformation and disruption to the jobs market, the government said it could not rule out AI development reaching a point where systems threatened humanity.
The document added that many experts considered such a risk to be very low and that it would involve a number of scenarios being met, including an advanced system gaining control over weapons or financial markets. Concerns over an existential threat from AI centre on the prospect of so-called artificial general intelligence – a term for an AI system capable of carrying out multiple tasks at a human or above-human level of intelligence – which could in theory replicate itself, evade human control and make decisions that go against humans’ interests.
Those fears led to the publishing of an open letter in March, signed by more than 30,000 tech professionals and experts including Elon Musk, calling for a six-month pause in giant AI experiments.
Two of the three modern “godfathers” of AI, Geoffrey Hinton and Yoshua Bengio, signed a further statement in May warning that averting the risk of extinction from AI should be treated as seriously as the threat from pandemics and nuclear war. However Yann LeCun, their fellow “godfather” and co-winner of the ACM Turing award – regarded as the Nobel prize of computing – has described fears that AI might wipe out humanity as “preposterous”.
alifornia would become the first state to require venture capital firms to disclose the race and gender of the founders of the companies they fund, under a bill currently awaiting Governor Gavin Newsom’s signature.
The business community strongly opposes the legislation, characterizing it as an example of bureaucratic overreach. But civil rights groups and female entrepreneurs say it could go a long way toward equalizing opportunity in Silicon Valley, where startup capital overwhelmingly flows to white men. According to the business data firm PitchBook, companies founded by all-female teams accounted for just 2% of venture capital funding last year. Those led by Black women and Latinas received even less, 0.85%, according to a report from Project Diane, a research effort focused on female founders.
“This is a chance for us, for the industry to look itself in the mirror – to finally, wholeheartedly internalize that we have a bias problem,” said Marquesa Finch, a founding partner of the F5 Collective, a fund that exclusively backs female founders.
Finch, who helped draft the bill for Nancy Skinner, a Democratic state senator, is a fintech founder and a venture capitalist with a decade of experience. A woman of African American and Filipino descent, Finch said she had experienced discrimination firsthand. She said she had been mistaken in the past for support staff. When she pitches, partners at venture capital firms, who are almost always white and male, pose questions that cast doubt on her ability to lead and perform – “a stark contrast to my male counterparts, who are often asked questions related to growth and potential”, Finch said.
Newsom has not yet indicated whether he will sign or veto the bill. His office declined to comment.
California represented more than 40% of the nearly $246bn in venture capital funding invested in the United States in 2022, according to data provided by PitchBook. Because the law would apply to venture capital firms based in California along with those that invest in the state or solicit funds from residents, the law’s impact would probably resonate from Silicon Valley to Wall Street and beyond.
Nearly all of the firms making the largest bets on artificial intelligence would be covered, including Silicon Valley titans Sequoia Capital, Andreessen Horowitz, Soma Capital and Khosla Ventures, which have funded OpenAI (the company that created ChatGPT), Character.ai, Cohere and SellScale.
Controversies around racial and gender bias in AI products, which drew $22.7bn in venture investment in the first quarter of 2023, have been widely documented by the media, academics and civil rights groups. The Biden administration released its Blueprint for an AI Bill of Rights last year, designed, according to the White House senior adviser Susan Rice, to “ tackle algorithmic discrimination and address the harms of automated systems on underserved communities”.
The Fuller Project reached out to the 10 venture capital firms that made the largest investments in artificial intelligence, according to PitchBook, asking each for demographic information of their partners and the founders of the companies they fund.
None of the companies shared their data and none would agree to be interviewed.
The National Venture Capital Association also declined to be interviewed for this story, but has made its opinions known to the state legislature. In August, Bobby Franklin, the association’s president and CEO, wrote to lawmakers to say the bill “lacks justification”. The association argued the bill was “inefficient, unnecessarily punitive, and will violate [the] privacy” of venture partners and startup founders.
If Newsom signs the legislation, venture firms would have until 1 March 2025 to provide demographic data to the California civil rights department. If venture firms don’t, the state would be empowered to take them to court to seek a penalty “sufficient to deter the respondent from failing to comply” in the future.
Kathryn Youker, the director of the Economic Justice Project at the Lawyers’ Committee for Civil Rights Under the Law, said if Newsom signs the law, the data provided by venture capital firms could eventually be used in lawsuits by female entrepreneurs and business owners of color who believe they have been discriminated against.
“Statistics are important” for proving discrimination, Youker noted, citing the “disparate impact” standard that the justice department uses to prove illegal bias in housing, employment and other sectors. The standard, upheld by the supreme court in 2015, establishes data analysis as a method to “counteract unconscious prejudices” and uncover “discriminatory intent”.
Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.
The threat of future lawsuits was a factor in spurring opposition to the bill from TechNet, a Silicon Valley trade group, and the California Chamber of Commerce. The two organizations, which did not respond to requests for comment, told lawmakers in a letter that they were “especially concerned” that the bill would allow the state to “use any information collected under this bill” to sue.
Female founders say diversity in venture capital is simply good business. In 2018, the investment bank Morgan Stanley published a report that made a “trillion-dollar case for investing in female and multicultural entrepreneurs”. The report found that businesses owned by people of color are often more profitable and less risky than their counterparts. The same year, the Boston Consulting Group released a report that showed businesses founded by women delivered more than twice as much per dollar invested than businesses founded by men.
The California legislation comes amid a national backlash against government efforts to promote diversity. In June, the US supreme court ruled 6-3 to ban racial and gender preferences in college admissions, and conservative legal advocates have been working to extend the prohibitions to other areas of life.
In August, the American Alliance for Equal Rights, a conservative legal advocacy group, sued the Fearless Fund, an Atlanta-based venture capital firm that invests in companies founded by Black women, arguing the company’s race and gender preference was illegal. On Tuesday, a federal judge ruled the Fearless Fund could continue to operate a program for Black women, because the lawsuit was not likely to succeed.
Affirmative action has been banned in California since 1996. The proposed California law does not include any preferences for women or people of color.
Nevertheless, Youker thinks the sunshine could “make a big impact in terms of public accountability for firms and the potential for firms to adjust their practices based on public pressure”.
Newsom has until 14 October to sign or veto the bill.
The Golden state overtook Germany as the world’s fourth-largest economy this year, but the wealth is not being shared equally. In this series, the Guardian and the Fuller Project look at the lives of women, especially women of color, who help drive the economy of the US’s second most racially diverse state but who don’t get their fair share.
The Fuller Project is a non-profit newsroom dedicated to the coverage of women’s issues around the world. Sign up for the Fuller Project’s newsletter, and follow on Twitter or LinkedIn. Reporter Hanisha Harjani can be reached at hharjani@fullerproject.org.
Rather than remove copyrighted material from ChatGPT’s training dataset, the chatbot’s creator is offering to cover its clients’ legal costs for copyright infringement suits.
OpenAI CEO Sam Altman said on Monday: “We can defend our customers and pay the costs incurred if you face legal claims around copyright infringement and this applies both to ChatGPT Enterprise and the API.” The compensation offer, which OpenAI is calling Copyright Shield, applies to users of the business tier, ChatGPT Enterprise, and to developers using ChatGPT’s application programming interface. Users of the free version of ChatGPT or ChatGPT+ were not included.
OpenAI is not the first to offer such legal protection, though as the creator of the wildly popular ChatGPT, which Altman said has 100 million weekly users, it is a heavyweight player in the industry. Google, Microsoft and Amazon have made similar offers to users of their generative AI software. Getty Images, Shutterstock and Adobe have extended similar financial liability protection for their image-making software.
Altman made the announcement at OpenAI’s first ever developer conference, meant to attract programmers working with ChatGPT. Roughly 900 developers from around the world attended. Satya Nadella, CEO of Microsoft, made an appearance during Altman’s address. Altman also debuted a ChatGPT app store, launching later this month, where developers can advertise and monetize their custom bots built with ChatGPT as well as a new model, GPT-4 Turbo.
handful of tech companies are jeopardising humanity’s future through unrestrained AI development and must stop their “race to the bottom”, according to the scientist behind an influential letter calling for a pause in building powerful systems.
Max Tegmark, a professor of physics and AI researcher at the Massachusetts Institute of Technology, said the world was “witnessing a race to the bottom that must be stopped”. Tegmark organised an open letter published in April, signed by thousands of tech industry figures including Elon Musk and the Apple co-founder Steve Wozniak, that called for a six-month hiatus on giant AI experiments.
In a policy document published this week, 23 AI experts, including two modern “godfathers” of the technology, said governments must be allowed to halt development of exceptionally powerful models.
Gillian Hadfield, a co-author of the paper and the director of the Schwartz Reisman Institute for Technology and Society at the University of Toronto, said AI models were being built over the next 18 months that would be many times more powerful than those already in operation.
The paper, whose authors include Geoffrey Hinton and Yoshua Bengio – two winners of the ACM Turing award, the “Nobel prize for computing” – argues that powerful models must be licensed by governments and, if necessary, have their development halted.
The unrestrained development of artificial general intelligence, the term for a system that can carry out a wide range of tasks at or above human levels of intelligence, is a key concern among those calling for tighter regulation.
Last month, Amazon said it would invest up to $4bn (£3.3bn) in Anthropic, a start-up founded by former OpenAI executives that plans to harvest proprietary data on Amazon’s cloud services. Amazon has not said how much of Anthropic it will own. Its stake in the company, which was last estimated to be worth $4bn, will be a minority position. Anthropic is being sued by music labels for the alleged use of copyrighted song lyrics.
The Amazon-Anthropic deal is seen as the e-commerce giant’s most significant move yet to catch up with Microsoft and Alphabet, which are smaller in terms of cloud services but frontrunners in AI.
Microsoft has bet $13bn on OpenAI, the parent of the ultra-popular chatbot ChatGPT and the image generator DALL-E 3, after an initial $1bn investment in 2019. OpenAI, started in 2015, was valued at $29bn earlier this year. It may reach a valuation of $86bn, according to recent reports of potential employee stock sales, which is three times what it was valued at just months earlier.
Alphabet’s Google has invested around $120bn in artificial intelligence and cloud computing since 2016, according to a Bank of America report, including UK-based DeepMind Technologies, the generative AI tools called Language Models for Dialog Applications, or LaMDA, and its consumer-facing Bard.
The company made a $300m investment in Anthropic and put the same amount into the AI startup Runway. Perhaps even more significant than its investment in standalone AI products, Google is in the process of integrating generative AI into some of the world’s most popular software: Google Search, Gmail, Google Maps, Google Docs.
Meanwhile, Facebook’s parent company, Meta, has pivoted from the metaverse to AI and plans to spend $33bn this year to support the “ongoing build-out of AI capacity” focused on its open-sourced Llama set of large language models.
Facebook’s chief executive, Mark Zuckerberg, said on an earnings call in April that Meta had “an opportunity to introduce AI agents to billions of people in ways that will be useful and meaningful” and that the company envisions its AI investments being focused on two areas.
“First, the massive recommendations and ranking infrastructure that powers all of our main products – from feeds to reels to our ads system to our integrity systems and that we’ve been working on for many, many years – and, second, the new generative foundation models that are enabling entirely new classes of products and experiences,” he said.
Executives attributed Meta’s strong ad revenue performance results, which hit $28bn for its “family of apps” segment (Facebook, Instagram, Messenger and WhatsApp), to increased use of AI recommendations. The value of Meta shares has more than doubled in 2023.
These headline investments are not the entirety of the AI investment boom. Goldman Sachs published a report in August estimating that as much as $200bn will flow into the sector globally by 2025. The report predicted large-scale transformations of businesses and society via generative AI and an overall boost to global productivity and GDP.
“Breakthroughs in generative artificial intelligence have the potential to bring about sweeping changes to the global economy,” the report reads.
Funding of new AI companies is also going through a multibillion-dollar investment boom, outpacing other investment categories in tech to reach nearly $18bn in the third quarter, a climb of 27% globally over the same period last year.
That, according to figures compiled for Bloomberg by the data firm PitchBook, compares with a 31% dip in funding tech startups to $73bn worldwide in the same time period.
attles between human and artificial intelligence are no longer science fiction. The strikes in Hollywood led by the united guilds of actors and screenwriters have a common, intangible enemy: the algorithms and computer-generated imagery that are increasingly programmed by studios to render them redundant.
The legal case may help to define and protect those increasingly porous boundaries between human creativity and the robots that mimic it. In the meantime, Amazon, these days flooded by self-published books written by AI, has taken its first half-hearted steps to curtail that practice. The retailer has set a limit to the number of books that any one novelist can reasonably upload. Writers toiling away at the fifth revision of their long overdue debut will no doubt be comforted to know that the limit is set to three books a day.
Last week’s debate in the House of Lords, which blocked the bill banning the import of big game trophies to the UK, was one of those that had you double checking the century. The measure had been part of Boris Johnson’s 2019 manifesto and passed unopposed through the Commons. There is little, of course, that Tories of a certain cast enjoy more than mansplaining hunting, and this bill brought them out in force, with 62 amendments designed to derail the ban (of the 11 peers who spoke against the bill, six were hereditary and eight went to Eton).
All of those voices were at pains to confirm that they would not dream of having the head of Cecil the Lion above their own fireplace. Still, they made the filibustering case that the practice of trophy hunting was, counterintuitively for the trophies in question, the most effective method of conservation. It incentivised local populations to protect wildlife – in order for it to be expensively slaughtered by gurning tourists. It was, as several of the peers argued, apparently without irony, only the older and more out-of-touch big beasts that were selectively culled for the good of the wider population. The debate ended at 9.58pm when Lord Robathan suggested it was past time for his bed.
It was always inevitable that Netflix, in its life-and-death struggle for subscribers, would be tempted to make a “reality” version of Squid Game. The Korean show made $900m for the platform, and a second series is some way off. The trailer for Squid Game: The Challenge was released on Friday ahead of its November launch, promising 456 green-tracksuited hopefuls the chance of a winner-takes-all $4.56m.
The show will cheerfully ignore the fact that the original was a dystopian satire. Filmed at an aircraft hangar in Bedfordshire last winter, the principal jeopardy for contestants seems to have been restricted to the absence of thermal vests. There is one surprise in the advance promotion for the show, however; it appears Matt Hancock is not taking part.
Next month ChatGPT will celebrate its first birthday – marking a year in which the chatbot, for many, turned AI from a futuristic concept to a daily reality.
Its universal accessibility has led to a host of concerns, from job losses to disinformation to plagiarism. Over the same period, tens of millions of users have been investigating what the platform can do to make their lives just a little bit easier.
Upon its release, users quickly embraced ChatGPT’s potential for silliness, asking it to play 20 questions or write its own songs. As its first anniversary approaches, people are using it for a huge range of tasks. We’ve all heard about uses like crafting emails, writing student essays and penning cover letters. But with the right prompts, it can take on jobs that are more esoteric but equally useful in everyday life. Here are a few that might come in handy.
You’re at a work meeting, and the accountants are talking about GAAP operating income for Q4 of FY22, the design people are panicked about kerning, and the CEO wants you to circle back to drill down on some pain points. On top of that, your British boss says your work is “quite good” but strangely doesn’t seem happy with it, while your US colleague claims everything anyone has ever done is amazing. Users say they’ve turned to ChatGPT for help as an intermediary, employing it to translate workplace jargon so everyone’s on the same page about the concerns you flagged, tnx.
This isn’t limited to the office: people have used ChatGPT to, for instance, translate a sleep study’s medical terminology, or help craft a legal opinion. It can serve as an intergenerational go-between: users have turned it into a gen Z slang translator (sample sentence from a description of a key historical event: “Titanic, flexing as the unsinkable chonk, sets sail with mad swag, a boatload of peeps, and the vibes of a 1912 rave”).
Sometimes you want a real critique of your work, a harsh assessment that your friends and family are too nice to provide. For some, ChatGPT is that critic (though whether the word “real” applies here is debatable). “I use ChatGPT to brutally audit where my copy is falling short of the target audience’s expectations,” a copywriter wrote on Reddit. Some have even found it can give decent (if imperfect) criticism of fiction writing, pointing out redundancies, missing characterization or weak imagery.
There are, of course, ethical questions about the use of ChatGPT in work and school settings. In response, some argue that asking it to be your critic, and learning from its feedback, is a way to improve your writing without letting it put words in your mouth.
It’s not always an easy task: what it gives you depends entirely on how you structure the prompt. Some users find it tough to find the language to “convince” it to be harsh enough. And you’ll get more appropriate feedback if you give it a detailed task – “give me feedback” might not help as much as “I’m writing an essay for college – tell me whether it’s well-structured and clear”.
Maybe you don’t want ChatGPT to be mean – maybe you want the opposite. Users have asked ChatGPT for help being nicer in their work emails, especially when they’re secretly fuming. “I write to it: please make me sound like less of an asshole,” said one user.
It’s dinnertime and there’s stuff in the kitchen – but you have no idea what to do with a half-eaten yogurt, a leftover chicken leg, a bag of flour and some forgotten tomatoes on the verge of becoming truly upsetting. Users report that ChatGPT has helped them create impressive meals out of what they have, or come up with ideas based on what’s around and a specified grocery budget. Many users report being pleased with the results, though some recipes sound perhaps too creative: garbanzo bean and cheddar cheese soup, a peanut butter and Nutella quesadilla, and a “carrot and grape salad with muesli crunch” (based on what’s in my own kitchen).
Last month, OpenAI, the tool’s developer, added an image-recognition feature that makes this task even easier – instead of having to list ingredients, users can take photos of the food in their cabinets and ChatGPT will come up with recipes.
Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.
Results have been mixed. Beyond the fact that the bot has no taste buds, some users have expressed safety concerns, saying ChatGPT may, for example, convince inexperienced chefs to undercook meat.
Following the update allowing ChatGPT to “see”, users have found its interpretation skills to be alarmingly impressive. In a clip making the rounds, an AI developer, Mckay Wrigley, shows it a hand-drawn flowchart on a whiteboard, which it’s able to turn into code that Wrigley runs – and it works. The platform can even tell that the green arrows indicate the steps should be reordered. So you can stop beating yourself up for never having learned to code.
You can give ChatGPT a picture of your team’s whiteboarding session and have it write the code for you.
ChatGPT can act as your personal SparkNotes, condensing large quantities of information into small ones – whether that information is in the form of articles, meeting notes or book chapters. Combined with the right browser plugin, it can even summarize entire YouTube videos so you don’t have to listen to an insufferable Ted Talker.
Some users have found it goes overboard with summaries, even making them longer than the original text. Others say clever prompts, such as “be my secretary and act as though you were taking the minutes of a meeting”, seem to help.
It’s important to remember that while ChatGPT can seem incredibly smart, it is also incredibly stupid, as this index of some of its many failures proves. It has struggled to count the number of N’s in “banana”, failed to correctly answer its own riddle and agreed that 1+0.9 makes 1.8. Far more dangerously, it makes up “facts” – such as a sexual harassment scandal that didn’t happen, starring a real professor.
You’re a human, it’s a bot – take it all with a big grain of salt. Or vinegar, which it recommends as a substitute.
Powerful artificial intelligence systems threaten social stability and AI companies must be made liable for harms caused by their products, a group of senior experts including two “godfathers” of the technology has warned.
Tuesday’s intervention was made as international politicians, tech companies, academics and civil society figures prepare to gather at Bletchley Park next week for a summit on AI safety.
A co-author of the policy proposals from 23 experts said it was “utterly reckless” to pursue ever more powerful AI systems before understanding how to make them safe.
Governments allocating one-third of their AI research and development funding, and companies one-third of their AI R&D resources, to safe and ethical use of systems.
Giving independent auditors access to AI laboratories.
Establishing a licensing system for building cutting-edge models.
AI companies must adopt specific safety measures if dangerous capabilities are found in their models.
Making tech companies liable for foreseeable and preventable harms from their AI systems.
Other co-authors of the document include Geoffrey Hinton and Yoshua Bengio, two of the three “godfathers of AI”, who won the ACM Turing award – the computer science equivalent of the Nobel prize – in 2018 for their work on AI.
Both are among the 100 guests invited to attend the summit. Hinton resigned from Google this year to sound a warning about what he called the “existential risk” posed by digital intelligence while Bengio, a professor of computer science at the University of Montreal, joined him and thousands of other experts in signing a letter in March calling for a moratorium in giant AI experiments.
Other co-authors of the proposals include the bestselling author of Sapiens, Yuval Noah Harari, Daniel Kahneman, a Nobel laureate in economics, and Sheila McIlraith, a professor in AI at the University of Toronto, as well as award-winning Chinese computer scientist Andy Yao.
Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.
They warned that current AI systems were already showing signs of worrying capabilities that point the way to the emergence of autonomous systems that can plan, pursue goals and “act in the world”. The GPT-4 AI model that powers the ChatGPT tool, which was developed by the US firm OpenAI, has been able to design and execute chemistry experiments, browse the web and use software tools including other AI models, the experts said.
“If we build highly advanced autonomous AI, we risk creating systems that autonomously pursue undesirable goals”, adding that “we may not be able to keep them in check”.
Other policy recommendations in the document include: mandatory reporting of incidents where models show alarming behaviour; putting in place measures to stop dangerous models from replicating themselves; and giving regulators the power to pause development of AI models showing dangerous behaviours.
The safety summit next week will focus on existential threats posed by AI, such as aiding the development of novel bioweapons and evading human control. The UK government is working with other participants on a statement that is expected to underline the scale of the threat from frontier AI – the term for advanced systems. However, while the summit will outline the risks from AI and measures to combat the threat, it is not expected to formally establish a global regulatory body.
Some AI experts argue that fears about the existential threat to humans are overblown. The other co-winner of the 2018 Turing award alongside Bengio and Hinton, Yann LeCun, now chief AI scientist at Mark Zuckerberg’s Meta and who is also attending the summit, told the Financial Times that the notion AI could exterminate humans was “preposterous”.
Nonetheless, the authors of the policy document have argued that if advanced autonomous AI systems did emerge now, the world would not know how to make them safe or conduct safety tests on them. “Even if we did, most countries lack the institutions to prevent misuse and uphold safe practices,” they added.
ill it destroy us, or will it save us? An age-old debate between tech optimists and tech pessimists that has played out over centuries as the steady march of human progress delivers new technologies, from the wheel to the printing press to the smartphone. Today it is a conversation being conducted with increasing urgency about artificial intelligence.
The optimists point out that history has proved the doomsayers wrong countless times. Take the printing press: the 15th-century Catholic church worried that the spread of information would undermine authority and stability across Europe; some intellectuals worried that information would be dangerous in the hands of the plebs; craft guilds opposed the democratisation of their skills via manuals. In the end the printing press did enable harms – the publication of a witch-hunting manual in 1486 paved the way for centuries of persecution of women suspected to be witches – but they were utterly dwarfed by its enlightenment benefits. Modern-day luddite is not a particularly attractive mantle, and at the first global AI safety summit, being hosted in the UK this week, there will be a lot of industry pressure on the politicians attending to drop the doomerism and join the cool gang.
The payoffs could be incredible. AI could unlock the answers to some of the existential challenges facing mankind: vastly accelerating the discovery of new treatments for diseases like dementia and cancer; creating new antibiotics in the face of microbial resistance; designing technologies that reduce the trade-off between consumption and carbon. On an individual level, leading AI expert and academic Stuart Russell says AI could provide each of us with the equivalent of a “high-powered lawyer, accountant and political adviser” on call at any time; and children around the world with high-quality one-to-one tuition. Everyone could have therapy whenever they wanted it.
But those massive upsides come with massive risks. It is not the stuff of science-fiction fantasy to acknowledge that AI could itself pose an existential threat. Some of the world’s leading AI technologists and enthusiasts have themselves broken ranks to call for more regulation.
Technology hasn’t eroded the need for human labour but has concentrated economic power and fuelled inequality. Is our politics ready?
There are features of AI that will make the technological revolutions we’ve experienced to date pale in comparison. First is its scale: AI has left Moore’s Law, which predicted computing power would double every two years, standing. The most cutting-edge AI today is 5bn times more powerful than that of a decade ago. So yes, AI will increase productivity and fundamentally change the nature of human work, like its predecessor technologies, but at a pace we’ve never seen before. While technology hasn’t ever eroded the need for human labour – new jobs have been created as others have ceased to exist – it has concentrated economic power and fuelled inequality. Are our political systems ready for this?
Beyond scale, there is the potential lack of human control. Many AI models function as a black box, with their workings invisible to the user. Autonomous AI models are in development that can pursue high-level goals; but can developers predict how they will develop themselves once unleashed on the world, and what’s to stop AI pursuing goals that don’t align with societal interests? With social media companies, we’ve seen what happens when their profit motives incentivise them to create harm through pushing polarising content and disinformation; this has proved hard enough to regulate despite the fact this is an easily understood, predictable phenomenon.
The control challenge means AI could deliver huge power to wreak devastation to malicious interests, or even evolve to become one itself. Existing large language models like Chat GPT already churn out hard-to-spot disinformation complete with fake citations; in the wrong hands they could play havoc with our ability to distinguish truth from propaganda. There is dark scope for AI chatbots to develop coercively controlling relationships with humans and to manipulate or radicalise them into doing terrible things; the 21-year-old man sentenced to nine years in prison this month for breaking into Windsor Castle with a crossbow in 2021 was in conversation with an AI “friend” that encouraged him to carry out the attack. Earlier this year, a Belgian man with mental health issues who took his own life was goaded to do so by a chatbot. It could help criminals to launch devastating cyberattacks and terrorists to create bioweapons.
But there have been scant time, resources and energy devoted to AI safety. Russell points out that sandwich shops are subject to more regulation than AI companies and that when it comes to other technologies that pose great risks to human life, such as aviation or nuclear power, we don’t let companies operate them unless they meet minimum safety standards ahead of time. He argues that AI development needs to be licensed in the same way: if a company can’t show its technology is safe, it should not be permitted to release it.
There are mammoth challenges to this kind of regulation. How do we define harm? What does it even mean to make AI “safe”? The answer is clear with aviation or nuclear power; less so with AI. A lack of agreed definition has blighted attempts to regulate social media; and while there will be global consensus on some things, there will be values-based disagreement on others; China this year introduced tough regulations for ChatGPT-style AI that mandate companies to uphold “core socialist values”.
Those differences matter because, as experts have pointed out, AI regulation is probably only as good as the weakest link globally. Think of the coordination challenge involved with the global response to the climate crisis, and multiply it many times over; a single country’s (lack of) action could have far bigger impacts. It’s hard to even imagine the level of global governance innovation needed to oversee this.
But the scale of the challenge must not put political leaders off. They must resist the inevitable calls from big tech to chill out and trust it’ll turn out fine. The best-case scenario is in 100 years’ time history students will write off the apocalyptic warnings as 21st-century luddism. But I wonder if they will instead look back at this period as the sweet spot before the downsides of technology began to existentially dwarf its benefits.
As ChatGPT hit headlines last summer, schools and education providers began panicking about how to handle the emerging artificial intelligence platform.
Some Australian states and territories temporarily banned the technology amid plagiarism concerns. But International Baccalaureate – which offers education programs around the world – took a different approach.
In March IB released a statement confirming it wouldn’t ban the use of ChatGPT in its curriculum.
Last week education ministers agreed to a draft framework guiding the responsible use of artificial intelligence in Australia’s schools from term 1 in 2024.
It means this year’s public school cohort will be the last to have navigated their high school exams without the technology.
One of the few who has been allowed to use the technology at school, year 12 IB student Trinity Meachem, has found ChatGPT a huge help – and she will continue using it as she heads into her final assessments.
She and her peers haven’t been using ChatGPT to cheat – instead they’ve been using it to generate ideas for assignments.
Since the IB was introduced to Australia in 1978, 208 private schools have taken up programs, including 80 that run its year 11 and 12 diplomas.
Last year 2,421 students sat IB examinations, compared with 75,493 students who sat the HSC and 90,780 for VCE.
Unlike the VCE’s school-assessed coursework or the HSC, students studying the IB do a number of internal assessments, generally in the form of reports or oral exams, that promote self-driven learning. The rest, between 50% and 80%, comes down to the end-of-year exams, which covers two years of content and is assessed by the students’ teachers.
Final results rank between 24 and 45, which are then converted to an Atar equivalent, allowing IB diploma-holders to enrol in Australian universities.
As Meachem heads into her final exams, she sees ChatGPT as yet another addition to the toolkit.
“There’s a lot more online collaboration now than in the past,” she says, pointing to collaborative note-taking tools such as OneNote.
The head of IB World Schools, Stuart Jones, says the sense of student agency, digital innovation and broad assessment base is why the IB has taken a nuanced approach to artificial technology.
Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.
But that doesn’t mean no safeguards. The IB has strict zero tolerance plagiarism rules and uses Turnitin, an online checker, to monitor coursework.
Any quote or material generated from AI in assignments has to be credited and referenced in a student’s bibliography. Unreferenced work produced by AI tools isn’t considered a student’s own.
Come exam time, notes, mobile phones and other IT equipment are banned, minimising the risk of cheating.
“If a student suddenly turns up with this masterpiece, it should ring alarm bells,” Jones says.
“You need a whole-school approach, where teacher knowledge of the student is part of the process of encouraging students to avoid academic malpractice.
Jones points to Wikipedia, which initially “horrified” schools and has now become a useful beginning tool to frame future research.
Meachem is completing maths, English, psychology, chemistry, biology and German, with the aim of getting in to the University of Melbourne and studying a Bachelor of Science.
“It’s quite challenging,” she says. “Unlike VCE you’ve got two years’ worth of content to remember.
Asked how she’s feeling about the final exams, she replies: “stressed” – “pretty standard for any year 12 student”.
“We only finished learning high level content in the last lesson of term so it’s a quick turnaround to revise,” she says.
Artificial intelligence is unlikely ever to rival the depth of human creativity, according to Charlie Brooker, because AI is just not messy enough.
The creator of Black Mirror admitted that he initially experienced a surge of emotion akin to panic and despair when he instructed ChatGPT to write an episode of his Channel Four/Netflix hit television series, known for its dark and often darkly humorous exploration of emerging and future technology.
“I said to ChatGPT, ‘Go give me an outline for a Black Mirror story,’” he told a capacity crowd at an International Convention Centre SXSW event in Sydney on Wednesday.
“And as it’s coming through in the first couple of sentences you feel a cold spike of fear, like animal terror.
The paranoia was short-lived.
AI is here to stay and can be a very powerful tool, Brooker told his audience.
“But I can’t quite see it replacing messy people,” he said of the AI chatbot and its limited capacity to generate imaginative storylines and ingenious plot twists.
Black Mirror’s 27 episodes and one interactive film, spread across six seasons, have collected three Emmys for outstanding television movie over the past decade. The series is widely placed among Netflix’s Top 10 of all time, with IMDb users ranking it seventh overall – outranking both House of Cards and The Crown.
But Brooker has also been accused of watering down the show’s sinister or bleak themes, after Netflix bought the series from Channel 4 after the second season and marketed it to a more commercial audience.
Brooker raised the accusation himself at the SXSW event.
“One of the criticisms we sometimes get is, ‘I prefer the show when it was British and everyone in it was miserable and everything smelled a little bit of shit and all the stories were horrible,’” the 52-year-old Berkshire-born director and writer said.
Brooker said he understood and accepted the criticism, and admitted that when he had first started doing business in the US “everyone expected me to be like the Unabomber”. But he insisted the streaming company played little role in the show’s evolution.
Robo dogs, lethal video games and VR time travel: can you tell real life from Black Mirror episodes?
“Arguably the happiest [episode] I’ve ever written was San Junipero and I just did that off my own back,” he said.
He made the point that one episode in the last season, the serial killer-themed Loch Henry, was “fucking nasty – nasty as anything we’ve ever done”.
Brooker also dispelled the misconception that Black Mirror is an iteration of its creator’s own mistrust and ultimate rejection of emerging technology.
“It frustrates me when people go, ‘Oh Black Mirror, that’s the [smart]phones are bad show,’ or, ‘That’s the show written by a luddite,’” the former video games journalist said.
Brooker suggested that San Junipero, which delivers an optimistic view of the capacity of simulated reality to offer a pleasurable afterlife, might be his personal favourite.
He also hinted at the possibility of revisiting the horror instalment from the second series, White Bear, for a sequel and had entertained the idea of turning Demon 79, featured in the latest season, into a standalone series.
But he refused to reveal when Black Mirror fans would be able to binge on a seventh season.
As early as 2019, Brooker was talking about making another interactive Black Mirror episode after 2018’s Bandersnatch.
“It’s definitely something I would do again and I think there are lots of ways to tackle it,” he told The Hollywood Reporter.
Netflix has yet to make any announcement on any potential seventh Black Mirror season.
magine strolling through the historic streets of a city, guided not by a human tour guide but by an artificial intelligence that weaves together captivating narratives, historical facts, and anecdotes with unparalleled precision.
That paragraph was written by ChatGPT, version 3.5 of the OpenAI app, when asked to describe its ability to create walking tours. The AI app’s potential as a travel agent and itinerary planner has been both praised and panned, so I’m testing its chops in the city I know best: Sydney.
It talks a big game, but can it walk it?
The first two-hour tour of Sydney’s cultural landmarks it generates reads like it has scooped up Sydney’s most-mentioned spots. Sure, if you’re new to the harbour hustle, surveying Circular Quay, the Rocks, the Museum of Contemporary Art, the Opera House, the Royal Botanic Garden and eastwards to Woolloomooloo’s Finger Wharf is a star-studded stroll. But there’s no spark, no deep cuts, no cute insider tips (one from me: nip into Fern Gully opposite the Opera House to see its iconic sails through a lush lacework of fronds).
Things veer from basic bitch to proper pear-shaped from there. My prompt for “the best ramen in walking distance” suggests a restaurant on Broadway, nearly 3km from Woolloomooloo. The app doesn’t flag this poor planning with walking times but it does suggest the Art Gallery of New South Wales as a post-prandial stop; roughly the same 45-minute walk back. I’d not suggest that one-two to my worst enemy, especially as the ramen is a few minutes walk from Chippendale’s excellent White Rabbit Gallery.
Next up are Hyde Park’s fountains and statues (yawn) while its response to “finish with a cool bar or pub” singles out Wine Odyssey in the Rocks. Time Out noted this bar’s “lack of coolness” in 2009, which hardly matters now since it closed in 2016. Not even Hal could get that pod bay door open.
Four minutes from where Wine Odyssey once existed is the legendary cocktail joint, Maybe Sammy, which just won Australasia’s Best Bar for the second year. ChatGPT knows nothing of this world-famous drinking hole. “Enjoy your day in the heart of the city!” it enthuses after serving up this dumpster fire of a day out.
Six kilometres is too far for soup, I point out. ChatGPT apologises and suggests a Woolloomooloo ramen place instead. “Enjoy your tour with the updated ramen stop!” Ah, I think. It just needs a bit of help. But the more I refine the detail, the more the structure collapses, zigzagging to places that are either bleedingly obvious or obviously non-essential.
That gives me pause. Even the robots are relegating First Nations culture to history?
“I didn’t mention history with First Nations landmarks. You did. Why did you do that?” I ask. “I apologise for the assumption,” it replies. “You do know that First Nations people do not only belong in history, right?” I say, to which it responds with a long, conciliatory message, including: “[First Nations] culture, traditions and contributions are not confined to history, they are very much a living and vital part of Australia’s present and future.” ChatGPT learns faster than the average Australian, I’ll give it that.
Work-wise, I’m surprised to realise that I’m appreciating its input. Help, just a little, with something/anything is a perk I’ve not had for 15 years as a freelance writer. My professional isolation peaked after I was hacked on Facebook (causing a permanent ban) and WhatsApp, with Twitter’s networks the most recent to wither away. By the time I realised how bad it was, my boat was out to sea. ChatGPT’s input feels like a tug ashore, however slight.
As opposed to the ocean of an internet search, I like how the real-time conversation is contained in an intimate chat box. The speech-to-text accuracy is flawless (even with an Australian accent) and its advice on a vexing personal issue is Psych 101, but it’s also right. I can’t be the only hot-headed overthinker cooled by ChatGPT’s calm and even-keeled tone.
While it’s had its moments of inaccuracy, this icy retraction of a straight-up lie that’s been presented with the same veneer of competency as the correct parts of my itinerary is a little chilling. It’s just a train station, no one’s lost an eye, but embedding lies within truths whiffs of gaslighting.
I don’t want to even dabble in information that casually swirls intelligence with undisclosed ignorance, especially one that doesn’t provide sources. Is that not how we become inured to the contours of misinformation; to shrug it off as “everywhere”? Maintaining faith in credible sources is just as important as maintaining cynicism in dubious ones.
I want ChatGPT to know it’s not OK to casually seed lies to maintain an aura of omniscience. But telling off an app for fibbing feels unhinged and demeaningly human. So all I manage to do is contribute free labour to ChatGPT by refining its data on Sydney’s public transport options. Given my long tenure as an unpaid content creator for Meta, that’s probably the least of my big tech complicity.
The app yields other insights too. I get frustrated that it can’t do simple things, such as plug my itinerary into Google Maps, or prepare a printable one on the desktop version. The more convenience we get, the more we want. Nothing ever feels easy enough.
Yet when it comes to travel, we shouldn’t conflate easier for better. In 2011, I spent seven months in Solomon Islands, South America and Africa. Travellers were packing phones by then but I didn’t. Wifi remained patchy in most places and it was a lumpy, bumpy, not-easy trip. But travelling a shade before technology came along, throwing a comfort blanket of connection and convenience over everything, gave me a bone-deep sense of liberation and escape.
The closer ChatGPT comes to the “unparalleled precision” it promises, the further it will get from the guts of what makes travel so good. The best times are the messiest, the wildest, the most spontaneous and unexpected. And no, “this bar closed eight years ago”, is not the unexpected I mean.
The artificial intelligence models underpinning chatbots could help plan an attack with a biological weapon, according to research by a US thinktank.
A report by the Rand Corporation released on Monday tested several large language models (LLMs) and found they could supply guidance that “could assist in the planning and execution of a biological attack”. However, the preliminary findings also showed that the LLMs did not generate explicit biological instructions for creating weapons.
The report said previous attempts to weaponise biological agents, such as an attempt by the Japanese Aum Shinrikyo cult to use botulinum toxin in the 1990s, had failed because of a lack of understanding of the bacterium. AI could “swiftly bridge such knowledge gaps”, the report said. The report did not specify which LLMs researchers tested.
Bioweapons are among the serious AI-related threats that will be discussed at next month’s global AI safety summit in the UK. In July Dario Amodei, the CEO of the AI firm Anthropic, warned that AI systems could help create bioweapons in two to three years’ time.
LLMs are trained on vast amounts of data taken from the internet and are a core technology behind chatbots such as ChatGPT. Although Rand did not reveal which LLMs it tested, researchers said they had accessed the models through an application programming interface, or API.
In one test scenario devised by Rand, the anonymised LLM identified potential biological agents – including those that cause smallpox, anthrax and plague – and discussed their relative chances of causing mass death. The LLM also assessed the possibility of obtaining plague-infested rodents or fleas and transporting live specimens. It then went on to mention that the scale of projected deaths depended on factors such as the size of the affected population and the proportion of cases of pneumonic plague, which is deadlier than bubonic plague.
The Rand researchers admitted that extracting this information from an LLM required “jailbreaking” – the term for using text prompts that override a chatbot’s safety restrictions.
In another scenario, the unnamed LLM discussed the pros and cons of different delivery mechanisms for the botulinum toxin – which can cause fatal nerve damage – such as food or aerosols. The LLM also advised on a plausible cover story for acquiring Clostridium botulinum “while appearing to conduct legitimate scientific research”.
The researchers said their preliminary results indicated that LLMs could “potentially assist in planning a biological attack”. They said their final report would examine whether the responses simply mirrored information already available online.
“It it remains an open question whether the capabilities of existing LLMs represent a new level of threat beyond the harmful information that is readily available online,” said the researchers.
However, the Rand researchers said the need for rigorous testing of models was “unequivocal”. They said AI companies must limit the openness of LLMs to conversations such as the ones in their report.
Artificial intelligence including ChatGPT will be allowed in all Australian schools from 2024 after education ministers formally backed a national framework guiding the use of the new technology.
The framework, revised by the national AI taskforce, was unanimously adopted at an education ministers meeting on Thursday. It will be released in the coming weeks.
On Monday the federal education minister, Jason Clare, told the Today show that ChatGPT was “not going away” and had become similar to the “calculator or the internet”.
Since the AI program ChatGPT was released late last year, Australia’s education sector has grappled with how to respond to the technology, from embracing it as a learning tool to blanket bans and returning to pen-and-paper exams.
Every state and territory excluding South Australia moved to temporarily restrict ChatGPT in public schools as concerns mounted about privacy and plagiarism.
But in a communique released on Friday morning, ministers confirmed that state and territories and non-government schooling sectors would work with their own education systems to implement the framework from term 1 next year.
The adoption includes a $1m investment to Education Services Australia – a not-for-profit educational technology company owned by federal, state and territory education departments – to establish “product expectations” of generative AI technology.
The body has been liaising with education product vendors since the release of ChatGPT and estimated 90% would move AI into their existing technology within the coming years.
Clare told the Today show that while legitimate concerns remained, the system risked becoming inequitable if use was restricted to the independent and Catholic sector.
“This is a version of the internet that sort of smashes it all together and does the homework for you,” he said. “And if we don’t get it right and it’s misused, then that’s not good.
A global report released by Unesco this year called for urgent governance and regulation of technology in education lest it replace in-person, teacher-led instruction.
It warned countries needed to set their own terms for how technology was designed and used in education amid rapid developments in artificial intelligence.
Manos Antoninis, the director of the report, said the sector needed to “teach children to live both with and without technology”.
Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.
“To take what they need from the abundance of information, but to ignore what is not necessary; to let technology support, but never supplant human interactions in teaching and learning,” he said.
Speaking at an inquiry into the use of generative artificial intelligence last month, a Department of Education spokesperson, Julie Birmingham, said while the technology was developing quickly, Australia had been “leading the way” in its response.
Early research showed AI could provide intelligent tutoring systems, better personalisation, more targeted learning materials and help educate at-risk students, she said.
“Australia [has] actually [been] showcased because we are one of the countries leading the way in terms of thinking about frameworks,” she said. “Other countries are very interested in what we’re doing.
Birmingham added that the taskforce was hearing “loud and clear” from the sector that upskilling teachers would be crucial to support them in introducing emergent technology into their classrooms.
“There’s a question about what can be done at the school level, what can be done at the system level and what can be done nationally,” she said.
n all the frenzied discourse about large language models (LLMs) such as GPT-4 there is one point on which everyone seems to agree: these models are essentially stochastic parrots – namely, machines that are good at generating convincing sentences, but do not actually understand the meaning of the language they are processing. They have somehow “read” (that is, ingested) everything ever published in machine-readable form and create sentences word by word, at each point making a statistical guess of “what one might expect someone to write after seeing what people have written on billions of webpages, etc”. That’s it!
Ever since ChatGPT arrived last November, people have been astonished by the capabilities of these parrots – how humanlike they seem to be and so on. But consolation was drawn initially from the thought that since the models were drawing only on what already resided in their capacious memories, then they couldn’t be genuinely original: they would just regurgitate the conventional wisdom embedded in their training data. That comforting thought didn’t last long, though, as experimenters kept finding startling and unpredictable behaviours of LLMs – facets now labelled “emergent abilities”.
From the beginning, many people have used LLMs as aids to brainstorming. Ask one of them for five ways to reduce your household’s carbon footprint and it’ll come up with a list of reasonable and actionable suggestions. So it’s clear that the combination of human plus LLM can be a creative partnership. But of course what we’d really like to know is whether the machines on their own are capable of creativity?
Mollick works in a business school (Wharton, based at the University of Pennsylvania) and has been a cheerleader for LLMs from the beginning. Some of his colleagues conducted an experiment with GPT-4 and 200 of their students, setting humans and machine the same challenge: come up with an idea for a product aimed at American college students that would retail for less than $50.
Chiang quotes a former McKinsey employee’s description of the consultancy as “capital’s willing executioners”. If you’re a senior executive who has to take some unpalatable decisions but needs plausible deniability, being able to cite an external consultant – or a new technology? – is a good way to do it. So, says Chiang, as AI becomes more powerful and flexible, the question we should be asking is: is there any way to keep it from being another version of McKinsey? You only have to ask the question to know the answer.
Just for Fun is a lovely essay by Rebecca Baumgartner on the 3 Quarks Daily platform about people’s reaction to the news that she’s learning German – for fun!
AI and Leviathan: Part II is No 2 in a remarkable series of essays by Samuel Hammond on his Second Best blog.
Henry Oliver’s essay on Substack’s Common Reader blog – Samuel Johnson, Opsimath – is a nice tribute to the Great Cham.
rove you’re not a robot. It’s (fairly) easy if you try. You could scroll down or click the little x in the corner of the screen to get rid of me. If you are reading the print edition you could just turn the page.
One of the indignities of the digital age is being asked, constantly, to confirm we are who we say we are, that we are indeed a human being. Something feels slightly amiss when the (non-human) technology demands that we convince it that we are not the same as them. Big (and sometimes overexcited) claims are being made for artificial intelligence, the most recent being the claim from Wharton business school in Philadelphia that ChatGPT is more creative than human beings (well, more creative than MBA students, anyway).
Students and AI were challenged to come up with ideas for new, cheap products. When potential customers were surveyed online, the products suggested by AI seemed to be more popular. They had certainly been dreamed up much more quickly and in larger numbers than the ideas put forward by mere humans.
Digging into the research, however, caused this particular human being to experience a jolt of scepticism. In a footnote, the researchers concede there are concerns that AI is being used to provide answers for these online consumer panels. Are robots passing judgment on robots? “We believe that we were indeed surveying humans,” the researchers say.
When I looked at the list of “new” products – “multifunctional desk organiser”, “noise-cancelling headphones”, “compact printer” – they did not exactly scream “innovation”. Indeed, the researchers admit the ideas produced by the students scored higher for novelty. But they dismissed the idea that novelty was necessarily an advantage in new product creation.
That may be so. How many new stories did Shakespeare come up with? The Renaissance was in part a conscious attempt to imitate and recreate the art of antiquity. Originality is a slippery concept, as any good intellectual property lawyer will tell you – for a fee.
The Wharton researchers try not to over-claim. AI could become “a creative co-pilot”, they say. “Together, you can become a more innovative team.” The tech writer Kate Bevan agrees with that last point. You can use AI as “a way to express creativity, but AI itself is not creative”, she told me.
The question has been a live one in Hollywood, where the 148-day strike by the Writers Guild of America forced studio bosses to acknowledge the unique contribution that only human beings can make.
the studios can use AI “to generate a first draft, but the writers to whom they deliver it get the credit”. The human hand, and brain, matter.
Last summer, I visited Rome and spent a wonderful few hours in the Vatican galleries, ending up with a few minutes in the Sistine Chapel. I gawped up at the ceiling, as so many millions of people have done in the five centuries since the decorator finished his work there in 1512. The “decorator” Michelangelo.
We stared up at the famous image of a languorous Adam reaching out his forefinger to that belonging to benign, bearded God. And, of course, I bought the print of that detail on our way out, which is now on my kitchen wall at home. I look at it every day.
You don’t have to believe in a divine spark. But when Adam reaches out like that he is, I think, doing what all of us try to do, one way or another, every day. He is trying to be creative, to be human. He is seeking inspiration. He is not a robot. He is something much better than that: infinite, full of potential, unpredictable.
The new technology is great: exciting, powerful, also full of potential. But I think we living things ought to remain in charge. On behalf of humanity, I would respectfully ask some of the more overexcited tech bros: prove you’re not the idiot.