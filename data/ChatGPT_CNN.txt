OpenAI said recent outages of its viral ChatGPT chatbot could be caused by targeted attacks on its servers.
A DDoS attack, or distributed denial of service, typically refers to an attacker that floods an internet server to disrupt normal traffic.
“We are continuing work to mitigate this,” OpenAI said.
Users on Wednesday were unable to access all of OpenAI’s tools and services and received a message that the platform was at capacity.
The company told CNN no user information was compromised.
The outage comes three days after OpenAI hosted its first developer conference, held in San Francisco. It was held nearly a year after the launch of ChatGPT, which helped renew an arms race among tech companies to develop and deploy similar AI tools in their products.
CEO Sam Altman said 2 million developers now use the platform, and about 90% of Fortune 500 companies are using the tools internally. It currently has 100 million active users.
At the event, the company unveiled a series of artificial intelligence tool updates, including the ability for developers to create custom versions of ChatGPT.
The technology behind viral AI chatbot ChatGPT just got a whole lot smarter.
During its first developer conference held in San Francisco on Monday, OpenAI unveiled a series of artificial intelligence tool updates, including the ability for developers to create custom versions of ChatGPT. It is also launching a digital store and cutting base prices for developers while also pledging to pay some who use OpenAI products on their platforms.
The event comes nearly a year after the launch of ChatGPT, which helped renew an arms race among tech companies to develop and deploy similar AI tools in their products. CEO Sam Altman said 2 million developers now use the platform, and about 90% of Fortune 500 companies are using the tools internally. It currently has 100 million active users.
Among the biggest changes announced is the introduction of GPTs, or customized versions of ChatGPT. Similar to plugins, GPT can connect to databases, be used in emails or facilitate e-commerce orders, the company said. On its website, it says GPTs could be used to help with math tutoring, train for a marathon or design stickers, without any prior coding experience. Users can try it out via chatgpt.com/create.
“Creating one is as easy as starting a conversation, giving it instructions and extra knowledge, and picking what it can do, like searching the web, making images or analyzing data,” the company said in a blog post.
OpenAI is also rolling out a GPT Store, starting later this month, to allow GPTs to become searchable. Similar to other app stores, they’ll be listed on a leaderboard, and the company will highlight useful tools across categories such as productivity, education and “just for fun.” The company said developers will also be able to earn money based on how many people are using their GPT.
Altman also showed off GPT-4 Turbo, the latest version of the technology that powers ChatGPT. He said it now can support input that’s equal to about 300 pages of a standard book, about 16 times longer than the previous iteration.
The platform will also have expanded knowledge through April 2023 “and continue to improve over time,” Altman said. “We are just as annoyed as all of you, probably more, that GPT’s knowledge of the world ended in 2021,” he added.
The company also said it is cutting the price for developers, to $0.01 for 1,000 input tokens, which is about three times cheaper than GPT-4. This would allow developers to save on overall costs when running high volumes of information through its systems.
Other improvements to the software include more modalities, such as enhanced text-to-speech with a more natural sounding audio with six preset voices to choose from.
Altman said OpenAI is also doubling down on safety and privacy, noting chats with the tools are not shared with builders.
The company also introduced Copyright Shield, which will allow OpenAI to step in and defend customers –- and pay costs incurred –- if copyright infringement becomes an issue. Rivals such as Google and Adobe have taken a similar approach.
In the meantime, OpenAI emphasized the latest announcements are only part of what’s still ahead.
You can now speak aloud to ChatGPT and hear the artificial intelligence-powered chatbot talk back.
OpenAI, the startup behind the wildly-popular chatbot, announced Monday that it is rolling out new features including the ability to let users engage in a back-and-forth voice conversation with ChatGPT.
The new voice features from OpenAI carry similarities to those currently offered by Amazon’s Alexa or Apple’s Siri voice assistants.
ChatGPT’s voice capability is “powered by a new text-to-speech model, capable of generating human-like audio from just text and a few seconds of sample speech,” Open AI said in the blogpost. The company added that it collaborated with professional voice actors to create the five different voices that can be used to animate the chatbot.
OpenAI also said on Monday that it’s rolling out a new feature that lets the bot respond to prompts featuring an image. For example, you can snap a picture of the contents of your fridge and ask ChatGPT to help you come up with a meal plan using the ingredients you have. Moreover, the company said you can ask the chatbot to focus on a specific part of an image with its “drawing tool” in the app.
The new features roll out in the app within the next two weeks for paying subscribers of ChatGPT’s Plus and Enterprise services. (Subscriptions to the Plus service are $20 a month, and its Enterprise service is currently only offered to business clients).
The updates from OpenAI come amid an ongoing AI arms race within the tech sector, initially spurred by the public launch of ChatGPT late last year. In recent weeks, tech giants have been racing to roll out new updates that incorporate more AI-powered tools directly into their core products. Google last week announced a series of updates to its ChatGPT competitor Bard. Also last week, Amazon said it was bringing a generative AI-powered update to its Alexa voice assistant.
OpenAI is releasing a version of its buzzy ChatGPT tool specifically for businesses, the company announced Monday, as an AI arms race continues to ramp up throughout corporate America.
OpenAI unveiled the new service, dubbed “ChatGPT Enterprise,” in a company blog post and said it will be available to business clients for purchase as of Monday. The new offering promises to provide “enterprise-grade security and privacy” combined with “the most powerful version of ChatGPT yet” for businesses looking to jump on the generative AI bandwagon.
Some of the early customers of ChatGPT Enterprise include fintech startup Block, cosmetics giant Estee Lauder Companies and the professional services firm PwC.
The highly-anticipated announcement from OpenAI comes as the company says employees from over 80% of Fortune 500 companies have already begun using ChatGPT since it launched publicly late last year, according to its analysis of accounts associated with corporate email domains.
Before the launch of ChatGPT Enterprise, a number of prominent companies including JPMorgan Chase had implemented temporary restrictions on workplace use of ChatGPT.
OpenAI did not publicly disclose the pricing levels for ChatGPT Enterprise, instead asking potential business clients to contact its sales team.
In July, Microsoft unveiled a business-specific version of its AI-powered Bing tool, dubbed Bing Chat Enterprise, and promised much of the same security assurances that ChatGPT Enterprise is now touting – namely, that users’ chat data will not be used to train AI models.
Microsoft also previously disclosed a multi-billion dollar investment into OpenAI. It’s not immediately clear how the dueling new AI tools for business will end up competing with each other.
Editor’s Note: A version of this article first appeared in the “Reliable Sources” newsletter. Sign up for the daily digest chronicling the evolving media landscape here.
News organizations are in a cold war with OpenAI.
While a shot has yet to be fired, some of the nation’s largest newsrooms are actively taking defensive measures to safeguard their content from ChatGPT, the groundbreaking artificial intelligence chatbot that is seen as a potential aggressor to an already struggling news industry.
A multitude of leading newsrooms have recently injected code into their websites that blocks OpenAI’s web crawler, GPTBot, from scanning their platforms for content. The Guardian’s Ariel Bogle reported last week that CNN, The New York Times, and Reuters had blocked GPTBot. But a Reliable Sources review has found several additional news and media giants have also quietly taken this step, including Disney, Bloomberg, The Washington Post, The Atlantic, Axios, Insider, ABC News, ESPN, and the Gothamist, among others. Publishers such as Condé Nast, Hearst, and Vox Media, which all house several prominent publications, have also taken the defensive measure.
Despite the posturing behind the scenes, none of the outlets that have taken the preventive measure of blocking GPTBot offered an on-the-record response when I reached out for comment on Monday. But the move to insert code disallowing OpenAI from drawing on their large libraries of content to train its ever-learning ChatGPT bot reflects the degree to which news organizations are spooked by the company’s technology and are quietly working to address it.
Danielle Coffey, president and chief executive of the News Media Alliance, told me on Monday that news organizations are indeed alarmed by the rapidly advancing technology. Coffey said that the News Media Alliance, which represents nearly 2,000 publishers in the US, believes newsrooms “are on solid legal ground when it comes to copyright protections.” Nevertheless, they’re apprehensive about how companies like OpenAI might further upend the already embattled news sector.
What exactly these media giants do next, however, remains to be seen. News organizations might feel they’re on solid legal ground, as Coffey told me, but there has yet to be any serious action taken against the OpenAI. Barry Diller has likely gone the furthest by taking a notably aggressive stance and signaling a future lawsuit. The NYT is also reportedly weighing whether to sue OpenAI. Meanwhile, the Associated Press went a different route, hammering out its own licensing deal with the A.I. developer, though it notably did not share key terms of the agreement.
If the issue is not resolved, enormous damage could be inflicted on the publishing industry, imperiling the information environment in the US and around the world even more than it is now. It’s not difficult to imagine how A.I. bots integrated into search, apps, and now-ubiquitous smart devices might put many newsrooms out of business, ironically doing so by using the very information they’ve derived from those newsrooms. Once these outlets are wiped from existence, a void of authoritative sources to train A.I. models would be created, and misinformation could be authoritatively passed along by confused bots feeding off a diet of bad information.
Despite the stakes being so high, the vast majority of news organizations are declining for now to publicly address the matter. Instead, they’re simply opting to discreetly lock their content in a protective vault until a more concrete battle plan can be hammered out. The news executive that I spoke to on Monday said that, at the very least, blocking GPTBot does make an unmistakeable point.
When college administrator Lance Eaton created a working spreadsheet about the generative AI policies adopted by universities last spring, it was mostly filled with entries about how to ban tools like ChatGPT.
But now the list, which is updated by educators at both small and large US and international universities, is considerably different: Schools are encouraging and even teaching students how to best use these tools.
“Earlier on, we saw a knee-jerk reaction to AI by banning it going into spring semester, but now the talk is about why it makes sense for students to use it,” Eaton, an administrator at Rhode Island-based College Unbound, told CNN.
He said his growing list continues to be discussed and shared in popular AI-focused Facebook groups, such as Higher Ed Discussions of Writing and AI, and the Google group AI in Education.
With more experts expecting the continued application of artificial intelligence, professors now fear ignoring or discouraging the use of it will be a disservice to students and leave many behind when entering the workforce.
Since it was made available in late November, ChatGPT has been used to generate original essays, stories and song lyrics in response to user prompts. It has drafted research paper abstracts that fooled some scientists and passed exams at esteemed universities. The technology, and similar tools such as Google’s Bard, is trained on vast amounts of online data in order to generate responses to user prompts. While they gained traction among users, the tools also raised some concerns about inaccuracies, cheating, the spreading of misinformation and the potential to perpetuate biases.
According to a study conducted by higher education research group Intelligent.com, about 30% of college students used ChatGPT for schoolwork this past academic year and it was used most in English classes.
Jules White, an associate professor of computer science at Vanderbilt University, believes professors should be explicit in the first few days of school about the course’s stance on using AI and that it should be included it in the syllabus.
Vanderbilt is among the early leaders taking a strong stance in support of generative AI by offering university-wide training and workshops to faculty and students. A three-week 18-hour online course taught by White this summer was taken by over 90,000 students, and his paper on “prompt engineering” best practices is routinely cited among academics.
Prompt engineering jobs, which typically require basic programming experience, can pay up to $300,000.
Diane Gayeski, a professor of communications at Ithaca College, is building ChatGPT into her students' assignments.
Diane Gayeski, a professor of communications at Ithaca College, said she plans to incorporate ChatGPT and other tools in her fall curriculum, similar to her approach in the spring. She previously asked students to collaborate with the tool to come up with interview questions for assignments, write social media posts and critique the output based on the prompts given.
Gayeski added that as long as there is transparency, there should be no shame in adopting the technology.
Some schools are hiring outside experts to teach both faculty and students about how to use AI tools. Tyler Tarver, a former high school principal who now teaches educators about tech tool strategies, said he’s made over 50 speeches at schools and conferences across Texas, Arkansas and Illinois over the past few months. He also offers an online three-hour training for educators.
“Teachers need to learn how to use it because even if they never use it, their students will,” Tarver said.
Tarver said that he teaches students, for example, how the tools can be used to catch grammar mistakes, and how teachers can use it to assist with grading. “It can cut down on teacher bias,” Tarver said.
He argues teachers could grade students a certain way even if they’ve improved over time. By running an assignment through ChatGPT, and asking it to grade the sentence structure on a scale from one to 10, the response could “service as a second pair of eyes to make sure they’re not missing anything,” Tarver said.
Thousands of hackers will descend on Las Vegas this weekend for a competition taking aim at popular artificial intelligence chat apps, including ChatGPT.
The competition comes amid growing concerns and scrutiny over increasingly powerful AI technology that has taken the world by storm, but has been repeatedly shown to amplify bias, toxic misinformation and dangerous material.
Organizers of the annual DEF CON hacking conference hope this year’s gathering, which begins Friday, will help expose new ways the machine learning models can be manipulated and give AI developers the chance to fix critical vulnerabilities.
The hackers are working with the support and encouragement of the technology companies behind the most advanced generative AI models, including OpenAI, Google, and Meta, and even have the backing of the White House. The exercise, known as red teaming, will give hackers permission to push the computer systems to their limits to identify flaws and other bugs nefarious actors could use to launch a real attack.
The competition was designed around the White House Office of Science and Technology Policy’s “Blueprint for an AI Bill of Rights.” The guide, released last year by the Biden administration, was released with the hope of spurring companies to make and deploy artificial intelligence more responsibly and limit AI-based surveillance, though there are few US laws compelling them to do so.
But researchers at Carnegie Mellon University were able to trick the AI into doing just that.
The findings are a cause for concern, the researchers told CNN.
Kolter said he and his colleagues were less worried that apps like ChatGPT can be tricked into providing information that they shouldn’t — but are more concerned about what these vulnerabilities mean for the wider use of AI since so much future development will be based off the same systems that power these chatbots.
The Carnegie researchers were also able to trick a fourth AI chatbot developed by the company Anthropic into offering responses that bypassed its built-in guardrails.
Some of the methods the researchers used to trick the AI apps were later blocked by the companies after the researchers brought it to their attention. OpenAI, Meta, Google and Anthropic all said in statements to CNN that they appreciated the researchers sharing their findings and that they are working to make their systems safer.
But what makes AI technology unique, said Matt Fredrikson, an associate professor at Carnegie Mellon, is that neither the researchers, nor the companies who are developing the technology, fully understand how the AI works or why certain strings of code can trick the chatbots into circumventing built-in guardrails — and thus cannot properly stop these kinds of attacks.
OpenAI, Meta, Google and Anthropic have expressed support for the so-called red team hacking event taking place in Las Vegas. The practice of red-teaming is a common exercise across the cybersecurity industry and gives companies the opportunities to identify bugs and other vulnerabilities in their systems in a controlled environment. Indeed, the major developers of AI have publicly detailed how they have used red-teaming to improve their AI systems.
“Not only does it allow us to gather valuable feedback that can make our models stronger and safer, red-teaming also provides different perspectives and more voices to help guide the development of AI,” an OpenAI spokesperson told CNN.
Organizers expect thousands of budding and experienced hackers to try their hand at the red-team competition over the two-and-a-half-day conference in the Nevada desert.
Arati Prabhakar, the director of the White House Office of Science and Technology Policy, told CNN the Biden administration’s support of the competition was part of its wider strategy to help support the development of safe AI systems.
Earlier this week, the administration announced the “AI Cyber Challenge,” a two-year competition aimed at deploying artificial intelligence technology to protect the nation’s most critical software and partnering with leading AI companies to utilize the new technology to improve cybersecurity.
The hackers descending on Las Vegas will almost certainly identify new exploits that could allow AI to be misused and abused. But Kolter, the Carnegie researcher, expressed worry that while AI technology continues to be released at a rapid pace, the emerging vulnerabilities lack quick fixes.
CNN’s Yahya Abou-Ghazala and Donald Judd contributed to this report.
We have seen enough of artificial intelligence to know the term is, quite often, a misnomer. Publicly available AI programs still churn out hilariously telling moments of non-intelligence as concepts like ears, the correct number of fingers, a natural voice cadence or cohesive complex thought continue to evade them. It’s fun to find these limits. It’s fun to convince ourselves that the computers are still a long way from swallowing our culture whole.
It’s even more fun to do it through misshapen, unholy craft projects.
ChatGPT, a publicly available language-learning AI, was not designed to create things like crochet or knitting patterns. However, since such patterns are a form of language, it is theoretically possible for the program to create one. Many curious crafters have tried their hand at this, with increasingly absurd results.
In addition to being hilarious, this exercise poses a very fascinating quandary: What happens when you ask a program specifically trained on language to create something outside of that sphere?
Let’s crochet some ChatGPT-generated patterns and find out.
I followed the crochet pattern exactly. Where there were inconsistencies in the pattern, like a miscount of stitches or a nonsensical instruction, I tried for the most good-faith solution.
I chose objects and concepts that are not very common for crochet projects. There have been many issues raised about the extent to which programs like ChatGPT generate ideas already created by others, and I wanted to get as close to the bare heart of ChatGPT’s creativity as possible.
This was supposed to be the control project: A simple object with a distinct shape that has been reiterated in innumerable crochet patterns across the internet.
That is exactly what ChatGPT gave me. Was it a banana? Not even in the barest of senses. Was it something an extremely limited intelligence system floating in binary goo would approximate as banana? Perhaps.
At first glance, ChatGPT’s crochet patterns look and read exactly like a crochet pattern. They even have chirpy little introductions, and the program can clearly mimic terms any crafter would recognize – such as “work a stitch.” It also knew how many 3D crochet projects start: As a circle.
However, once the instructions progressed past a few common beginning stitches, the project usually devolved into one of two things: spheres, or complete nonsense. Here, it knew enough to provide instructions for a banana and a peel. Everything after that was just spheres.
While ChatGPT can create a linguistically cohesive pattern, it did significantly less well at specifying the “art” of it all, like how to assemble the work in any way that wouldn’t break the bounds of Euclidean geometry. In a traditional crochet pattern, instructions for assembly would note specific places and methods to attach pieces, along with photos or notes on particularly tricky steps.
In the introduction to the instructions, ChatGPT did flex its knowledge that Baby Yoda is also known as “The Child.” It then proceeded to render The Child as, you guessed it, a series of spheres.
This is a good time to introduce a little more background about how ChatGPT works and why it struggles so much with things like crochet patterns.
A narrow AI is trained on a specific skill. In ChatGPT’s instance, the skill is to reply in a way that sounds as human as possible. Theoretically, it would be possible to create AI that generated amazing crochet patterns, but that would require very specific training and programming.
Fair warning: This is the one that broke me. Everything you will see after this is an abomination, an attack on the natural order that should chill you to your core.
At least it wasn’t a sphere.
The pattern instructed me to create a beginning chain of stitches, and then build each row of stitches on top, which is how many flat crochet patterns like blankets are created. So far, so good. I started with 14.
The problem quickly became apparent when the instructions then told me to create five stitches into each existing stitch.
Do you see it? It’s barreling down on us, like an oncoming, ever-expanding train.
A few rows later, it gave me the same instructions. A few rows later, it repeated the instruction again: Five stitches in each stitch.
By the third row of increased stitches, I was no longer making 14 stitches into a flat shape. I was making 1,750 stitches into something that resembled a creeping coral reef. ChatGPT, with its nonexistent understanding of things like time, amounts of yarn, or the bounds of human sanity, had given me the crochet version of a fractal. Had I completed the pattern as written, I would be making more than 35,000 stitches into each row. Reasonably, I stopped at 1,750.
As I worked, I wondered whether I was unintentionally weaving a crude model of the universe. I pondered what equation I would need to solve to predict the way time, or maybe space, would arrange itself as it burst exponentially outward, row by row, and folded in on itself. If I knew how, could I locate myself here, now, nestled in some hidden crevasse?
No, you fools. It’s Antarctica.
I’ll admit, this was optimistic of me. I thought something with a very specific and simple shape would be easier for ChatGPT to approximate, and a very recognizable and uniquely-contoured building like Dubai’s Burj Al Arab seemed like the perfect mix of spatial simplicity and crochet pattern obscurity.
At least it wouldn’t be a sphere!
I saw potential in the Burj Al Arab test, and decided to choose another building that I thought for sure would break ChatGPT’s habit of rendering everything spheroid. The Burj Al Arab’s neighbor, the Burj Khalifa, seemed like a good choice. It’s so pointy! So tall!
Did you say a pointy tall sphere? ChatGPT can do that!
Now, if you’ve never been close to tears while crocheting something, I can say it is a singular experience. What was truly vexing about ChatGPT’s instructions, what was so nonsensical that it approached the beautiful entropy of human thought, was how the program perceived the concept of a spire and its relationship to the building it had me create.
That is not it, at all.
Maybe three-dimensional crochet is too much for a language-learning AI to handle. Maybe going all the way back to the basics would help it create something, just a single item, bearing even the foggiest resemblance to my prompts.
The initial result of this prompt was so nonsensical, I regenerated the response. Then I tried again. Creating these objects one after the other was like watching something struggle to be born. Eagle-eyed crocheters may recognize snippets of what could have been an intelligible heart pattern. By the fourth iteration, it was even producing a passable approximation of the shape. Was it learning, or, like a throng of monkeys on typewriters, was it just getting lucky?
I generated a few more patterns from this prompt and can assure you, it did not get any better.
If there is one piece of wisdom I gleaned from this experiment, it’s that human intelligence is fundamentally interdisciplinary. Language bleeds into sight, which tangles with memory or personality and so on. Artificial intelligence programs don’t really work that way. It may be able to do one thing really well – maybe even a few things. But once you push it past its assigned skill, it’s blobs all the way down.
OpenAI, the company behind the viral ChatGPT tool, has been hit with a lawsuit alleging the company stole and misappropriated vast swaths of peoples’ data from the internet to train its AI tools.
Moreover, this data scraping occurred at an “unprecedented scale,” the suit claims.
OpenAI did not immediately respond to CNN’s request for comment Wednesday. Microsoft, a major investor into OpenAI, was also named as a defendant in the suit and did not immediately respond to a request for comment.
“By collecting previously obscure personal data of millions and misappropriating it to develop a volatile, untested technology, OpenAI put everyone in a zone of risk that is incalculable – but unacceptable by any measure of responsible data protection and use,” Timothy K. Giordano, a partner at Clarkson, the law firm behind the suit, said in a statement to CNN Wednesday.
The lawsuit seeks injunctive relief in the form of a temporary freeze on further commercial use of OpenAI’s products. It also seeks payments of “data dividends” as financial compensation to people whose information was used to develop and train OpenAI’s tools.
OpenAI publicly launched ChatGPT late last year, and the tool immediately went viral for its ability to generate compelling, human-sounding responses to user prompts. The success of ChatGPT spurred an apparent AI arms race in the tech world, as companies big and small are now racing to develop and deploy AI tools into as many products as possible.