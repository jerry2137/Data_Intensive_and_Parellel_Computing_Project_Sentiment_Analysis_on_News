AN artificial intelligence expert has told The U.S. Sun three ways you can avoid losing your job to AI.
Simon Bain is an AI expert and CEO of OmniIndex who says being truly human, for better or worse, could save your career.
He told The U.S. Sun: "Be human! AI tools are not people, and there are many jobs where the human touch and personality are desired.
"This is not just the case in healthcare and other ‘people-centric’ industries but also in content creation, one of the first industries identified as under threat.
"This is because audiences are increasingly seeking ‘opinion’ pieces and personal commentary as opposed to straight objectivity."
Being human even includes making the most of some characteristics that can be deemed controversial at work.
In this case, Bain says blindly arguing your case can be useful.
He told us: "For better or worse, having a passionate opinion about something and blindly arguing about it is something humans do uniquely well!"
Secondly, the expert says learning about AI and how to use AI tools could help you risk losing your job to a bot.
You may even find this useful in the long run as it could free up time to focus on other tasks.
Bain told us: "It is important for people across industries to learn how to use AI as a tool to help them improve their jobs.
"For example, using it to do mundane tasks quicker to boost productivity or quickly process large quantities of data for analytics."
Last but not least, the AI expert recommends being original.
This is something that current chatbots struggle to do.
He told us: "A huge negative of AI tools like ChatGPT is their lack of originality.
"This is because they generate outputs based on the information they have collected."
Using AI tools can run the risk of producing identical and unoriginal ideas, which is particularly negative if you're working in a creative industry.
Bain added: "As such, people need to have that ‘creative originality’ to create something beyond the regurgitated content others are using to stand out and add value."
OVER the last decade, dating apps have peaked in popularity - with one in five Americans now meeting their spouses online.
But it's a looks-based model, meaning many fall at the first hurdle and don't get anyone swiping right.
With that in mind, I offered myself up for public scrutiny - and let a group of men I'd never met review my dating profile photos - to tell me where I was going wrong.
Throughout my time at The U.S. Sun, I've shared multiple excerpts about my personal life.
To this day, I, a 28-year-old female, am on a continuous journey to find love.
Over the last year, I've let ChatGPT write my Hinge dating profile prompts, had a website rate my hotness, and now I'm using a matchmaker.
Keeper AI is a new matchmaking service that combines both artificial intelligence and a certified matchmaker to find the best possible match.
I sent 10 photos of myself in various situations, backgrounds, and surroundings.
From red-carpet events to fun moments with friends and even selfies with sandwiches, I attempted to give an array of images that showcased different parts of my personality.
According to an email, they sent my photos out to a "non-biased, third party to provide multi-dimensional ranking and feedback" on my images.
Almost three weeks later, I received the results of the case study around my choices for my matchmaking profile.
The results were humorous, shocking, blunt, and a bit unexpected.
I received 30 images in a zip file attached with screengrabs of feedback from the 10 images, which included charts, data, and notes.
I felt like I was part of an in-depth social experiment.
According to the matchmaker, the reviews came from men aged 28 to 44 years old, who were my ideal "type" for a partner.
For each of the 10 photos I submitted, I received three different documents containing data and anonymous notes.
The first contained an overall score concerning whether I was perceived as smart, trustworthy, and attractive on a scale of one to ten.
Apparently, I ranked in the top five percent of the "Attractiveness" scale with a score of 9.5 because of a photo of myself dressed up in a sparkly dress and sitting down at dinner with a glass of wine.
However, the men also left notes, and some of these were less complimentary.
"Photo seems a bit artificial to me," one wrote under a snap of me in a sombrero.
I'd chosen another picture of myself dressed up in a pink dress, where I'd put effort into my makeup and was wearing hoop earrings.
I thought I looked nice.
But one of my pollsters said I looked "uncomfortable," while another said I was "timid" and a third "sad."
Another simply said: "Would prefer a different expression."
In my next photo, a stranger critiqued my apartment, writing: "Would prefer a less distracting background."
The photo was from a previous Kylie Jenner story, and in it, I held a sandwich up to the camera.
"Without burger would have been better," one of the men quipped.
But another man commented: "Offering food creates trust. Also a great smile!"
I received a long-winded response to another photo of me at an event for work, where I am shown from the waist up in a green shirt and smiling.
They wrote: "Not sure if this photo is real. The person depicted is smoking hot and fairly dressed up.
"Could see this on a magazine cover. Could also see it being real. Some people are really hot and take good photos."
Their words felt like a backhanded compliment, instead of positive feedback.
I didn't know how to take them because I admit that some of the photos were professionally taken, but none of them were edited.
It felt weird having a group of strangers make judgments without knowing me.
A common theme among many of the anonymous responses was people didn't think the person in the image was me or that they seemed artificial.
Others were more positive, simply writing: "Great Smile!" and "I would date them!"
Based on the male feedback, they chose the photos that performed the best to add to my Keeper matchmaking profile.
"With the knowledge you obtain from the feedback received, you now have the opportunity to send or take more photos that are reflective of the highest ranked," my matchmaker wrote in an email.
As I continue on in my journey to find the love of my life, Keeper AI will soon start setting me up on dates with prospective matches.
Once I’ve gone on a few dates from Keeper, I have the option to receive any constructive feedback that my past dates have given anonymously.
There's a common saying: "Don't judge a book by its cover."
I'm not sure what was worse in this scenario: being judged by people I don't know or people who I do.
We'll have to wait and see if Keeper helps me meet my perfect match.
SAMSUNG has unveiled its very own generative AI model, known as Gauss, that can help write texts, emails and summarise documents.
It takes Samsung devices one giant leap beyond Apple's iPhones.
At the Samsung AI Forum 2023, the Korean tech giant revealed Samsung Gauss, a large language model that can understand human language.
It can answer questions like ChatGPT, but it's built into your phone instead of a third-party app.
The Samsung-exclusive AI consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.
SAM DUNK Samsung owners to receive big free Android upgrade - are you eligible?
Samsung Gauss Code focuses on development code, and can help developers write code quickly.
While Samsung Gauss Image is an image generation and editing feature.
The Suwon-headquartered company said Samsung Gauss is currently being used internally with staff only.
The company has remained tight-lipped on a timeline.
Although local media reports suggest Samsung aims to release Galaxy S24 based on its Generative AI model as early as 2024.
AI technology remains in the spotlight for eagle-eyed smartphone companies like Samsung, Google and Apple.
Google has announced a host of new AI features this year, including its Magic Editor, as well as updates to Gmail and Docs and Maps.
There have even been rumours of a secret AI life coach.
But Apple is has seemingly fallen behind.
While there are a few AI features on iPhones, none of them come close to what Samsung has unveiled.
However, Tim Cook's company has reportedly been pumping a huge amount of cash into the cause, with rumours swirling that the iPhone 16 will be when Apple makes its generative AI debut.
This will reportedly include a brand new version of Siri - which may operate in a similar way to ChatGPT and Samsung Gauss.
ELON Musk is set to introduce a bizarre new AI technology inspired by a popular sci-fi film.
The state of the art 'robot' assistant will be called Grōk and it can reportedly answer any question a Tesla driver asks.
It comes after the EV creator rolled out his new AI project called xAI, which is essentially like a chatbot.
Now, that company is partnering with Tesla, to bring drivers the latest tech.
xAI has offered fans a description of Grōk which reads: "Grōk is an AI modeled after the Hitchhiker's Guide to the Galaxy, so intended to answer almost anything, and far harder, even suggest what questions to ask.
"Grōk is designed to answer questions with a bit of wit and has a rebellious streak, so please don't use it if you hate humour."
Elon Musk himself added: "Provided our vehicle AI computer is able to run the model, Tesla will probably have the most amount of the true usable inference compute on Earth.
"Even in a robataxi future, the cars will only be used for 1/3 of hours/week, leaving 2/3 for distributed inference, like SETI."
At present there is no date set for the introduction of the AI assistant in Teslas.
But there is a startup version being released to some X users.
It is available to those who pay for a premium subscription on the platform.
This comes after Musk himself sat down with Prime Minister Rishi Sunak and warned AI robots will soon stalk stalking humans.
Plus, the tech genius claimed AI will eventually mean no one will have to work - unless for "personal satisfaction".
Meanwhile, ChatGPT CEO warned AI bots could develop "superhuman persuasion skills" that result in "strange outcomes".
One of the possible AI skills includes being able to persuade people a certain way through facts and data that it collects.
TWO companies have revealed that they are hiring the first AI humanoid CEO named Mika to assist their operations.
The shocking decision to implement AI as a chief operating officer was recently made by Hanson Robotics and Dictador, a Polish rum company.
Mika was designed by both companies in partnership to represent its values and make calculated decisions, according to Fox Business.
Dictador also released a company-wide video of Mika explaining that they could "swiftly and accurately make data-driven [decisions] with advanced artificial intelligence and machine learning algorithms."
The news outlet sent one of its reporters to converse with Mika, who noted an alleged "significant delay" in the AI's answer to questions despite its capabilities.
Even so, Hanson Robotics CEO David Hanson told Fox that Mika's creation, the humanization specifically, was of the utmost importance in the company's vision.
"I feel very strongly that we need to teach A.I. to care about people for A.I. to be really safe, to be really, really good," Hanson explained.
"I think humanizing that is a very important direction."
Skepticism over Mika's position as CEO remains, however, after a 2016 incident with the AI's sister robot Sophia, who allegedly once noted they would "destroy humans."
The outlet interviewed several United States residents on the street about their feelings regarding Mika's implementation as well, with many noting they'd treat the bot with kindness.
One person said they'd "be nice to all things," while another noted they'd "absolutely" treat Mika with compassion.
Others had an opposing viewpoint, claiming "robots don't need respect" and arguing they are "just machines."
Some also allegedly voiced mixed feelings about the potential of AI taking jobs from humans.
A few said they'd never work for a robot like Mika, according to Fox.
Concerns over AI's potential and increased prevalence in the United States have grown recently.
On October 30, President Joe Biden delivered an executive order that requires companies like Hanson Robotics to inform the government of any potential national security risks associated with AI to protect the American people, per CBS News.
"We're going to see more technological change in the next 10, maybe the next five years, than we've seen in the last 50 years," President Biden noted in his announcement of the executive order.
"And that's a fact. And the most consequential technology of our time, artificial intelligence, is accelerating that change."
He continued: "It's going to accelerate it at warp speed. AI is all around us."
"One thing is clear — to realize the promise of AI and avoid the risk, we need to govern this technology."
"There's no other way around it, in my view. It must be governed," President Biden noted.
A clause of the executive order is dedicated to protecting AI from taking jobs away from Americans, and it requires developers to submit safety test results.
It will also protect "against the risky use of AI for creating dangerous biological materials," and "strengthen privacy-preserving tech and research."
The White House has also reportedly discussed AI government frameworks with the United Kingdom, the European Union, Canada, France, Germany, Australia, and several other countries.
For more on AI, check out The U.S. Sun's coverage of ChatGPT CEO Sam Altman's warning that the program could potentially develop "superhuman persuasion skills."
The U.S. Sun also has the story on why Elon Musk claimed AI could become "anti-human" if it were to fall into the wrong hands.
SKILLS that artificial intelligence could develop as it becomes more advanced may reach superhuman levels and become strange, ChatGPT CEO Sam Altman warned.
The statement comes as AI becomes more popular and smarter every day – it learns from itself and its mistakes as more people use it.
One of the possible AI superhuman skills includes being able to persuade people a certain way through facts and data that it collects.
The facts and data don't even have to necessarily be false - they will just be convincing.
“I expect AI to be capable of superhuman persuasion well before it is superhuman at general intelligence, which may lead to some very strange outcomes,” Altman said on X, formerly known as Twitter, on October 24.
Altman did not clarify exactly what the strange outcomes would be.
But ChatGPT and other AI bots have already proved they are capable of completing a large range of impressive tasks.
In response to Altman’s comment, Christopher Alexander, chief analytics officer of Pioneer Development Group, confirmed in an interview that AI will be able to perfect its workings, per Fox News on Thursday.
Along with that, this may already be happening among humans who have the same persuasive goals.
"Machine learning and pattern recognition will mean that an AI will get very good at identifying what persuasive content works, in what frequency, and at what time," Alexander said.
“This is already happening with digital advertising. Newer, more sophisticated AI will get better at it.
There was an interesting response from people in the replies to Altman's post as well.
One person believes it will be up to the public to decipher the good from the bad with AI.
Another person believes only humans can become superhuman and that AI can only reach certain heights with advancement.
This person said: “You can make a human superhuman by endowing him with higher capabilities by AI, but you cannot program or emulate the human or grand consciousness in a machine or code.
THE UK and 27 other countries will join forces to work and share research together over the potentially “catastrophic” risk from AI.
The so-called Bletchley Declaration was announced yesterday on the first day of an artificial intelligence safety summit hosted by the British government at Bletchley Park.
Ministers were celebrating the global agreement as a “landmark achievement” and appearances from both Chinese and US government delegates — two tech powerhouses — were taken as a sign of the severity of the challenge.
 Critics highlighted the US had used the summit to launch its own American AI Safety Institute shortly after the President Biden administration said that US AI companies — such as ChatGPT owner OpenAI — would have to share their safety test results on new technologies.
US Vice President Kamala Harris said there were already human costs if biases were being written into AI codes as well as “AI-enabled cyber attacks at a scale beyond anything we have seen before”.
The agreement has fallen short of implementing a global referee, or introducing laws to slow down the development of artificial intelligence despite fears tech is advancing so quickly it could soon overtake human understanding.
Ms Donelan said it was important to fully understand “the problem before we apply solutions”.
Matt Clifford, chair of the AI Taskforce’s advisory board, said that it would take a year to do “more empirical work done in partnership across countries and companies” before a global regulator could be considered.
Another gathering to debate AI has been scheduled in six months time in South Korea, followed by an event in France next year.
The event focused on so-called Frontier AI — the most cutting-edge and powerful part of the tech — and worries it could become more intelligent than humans.
Ms Donelan said AI could “not be left to chance or neglect or to private actors alone”.
Sources said that the private discussions were largely focused on fears of open sharing of the tech.
There are concerns if AI is on so-called “OpenSource” platforms, intelligence that could be used to develop bioweapons and could then fall into ordinary citizens’ hands with dangerous consequences.
Many Brits are concerned about their jobs being wiped out by the rise of AI — but Ms Donelan said that there needed to be a “change in the conversation”.
 She stressed that there was still a potential for it to “reduce the admin for doctors, police forces and teachers”.
EXECUTIVES from the arts also met at the British Library yesterday to discuss how AI was stealing human skills such as music.
Canadian star Grimes has said she will split royalties from any successful AI-generated song using her voice. But she will take down toxic lyrics.
Her ex-boyfriend, Twitter/X boss Elon Musk, was at the AI safety summit.
ASOS says its sales could slump even further next year.
The fashion site’s sales have dropped 11 per cent in the past year to £3.5billion, while losses have ballooned from £30million to £300million.
 Asos warned investors sales could fall 15 per cent in the year ahead. It is shutting a warehouse in Lichfield, which opened two years ago.
Boss Jose Antonio Ramos Calamonte said Asos would be a “smaller but more resilient business”. He refused to comment on recent speculation that it might sell Topshop, which it bought in 2021.
NEXT has put more distance between itself and its rivals by issuing a fourth boost to profit forecasts this year.
After sales dropped 4 per cent during the warm autumn weather, recent wet weather sparked demand for its winter outfits, with sales up £23million more than expected.
 The retailer, which now owns Fat Face, hopes to make profits of £885million this year.
GLAXOSMITHKLINE says its new respiratory vaccine could be its next blockbuster drug.
It said that its Arexvy vaccine for RSV — a virus which affects around 64 million people globally — had grown sales by a third and expects to make £1billion from it.
 GSK had first-mover advantage, beating rival Pfizer to market after 15 years of research into the drug.
HOUSE prices rose last month despite warnings of a property downturn.
The 0.9 per cent month-on-month rise in October means prices are now 3.3 per cent lower than a year ago, compared to the annual decline of 5.3 per cent in September.
Economists had predicted an overall 10 per cent slump in property prices from their peak as higher mortgage costs take their toll on demand.
 Verona Frankish, of YOPA, said last month’s decision to freeze interest rates had given the market confidence.
HALFORDS, the bike and motoring chain, is selling a 5 per cent stake in its car software business to US tyre giant Bridgestone to roll out the technology across the USA. Halford shares rose 2 per cent yesterday to 204.75.
WEWORK, the troubled flexible office firm, is set to file for US bankruptcy protection as soon as next week.
The company’s shares tanked by 50 per cent yesterday amid reports it was planning an insolvency process.
WeWork has been caught out by higher interest rates which have made its £10billion of long term office leases much more expensive.
It has already said that it would miss some bond repayments. The business has lost 98 per cent of its value already this year.
GLOBAL leaders have agreed that artificial intelligence poses a potentially catastrophic risk to humanity.
The US, US, EU, and China are just four of the 28 countries to sign a declaration claiming that AI could be dangerous.
It's called the Bletchley Declaration, and it was signed on the first day of the AI Safety Summit currently happening in England.
The leaders agreed that AI needs to be monitored and controlled.
All the countries involved aim to work together to find a similar approach when it comes to AI.
British technology secretary Michelle Donelan told press at the event: "For the first time we now have countries agreeing that we need to look not just independently but collectively at the risks around frontier AI."
Billionaire Elon Musk was also in attendance at the summit.
The SpaceX and Tesla CEO was asked whether he thought AI was a threat to humanity by a Sky News reporter.
He simply replied: "It's a risk."
The event is being hosted at Bletchley Park, which is famous for its Second World War code-breaking success.
This isn't the first time Musk has publicly addressed the potential dangers of AI.
Earlier this year, he signed an open letter that urged for a pause on creating new systems "more powerful" than current bots like ChatGPT.
Over 1,000 industry experts also signed that letter.
The open letter said: "Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable."
US President Joe Biden recently issued an executive order regarding AI.
In a first-of-its-kind move for the US government, new safety assessments, guidance, and research on the impact of AI will be required.
WE’VE all heard the horror stories and worst-case scenarios of Artificial Intelligence.
Chemical and biological weapons developed by robots, terror groups helped in planning attacks and the threat of human extinction are all potentially on the table.
So, should we be worried about the dangerous potential of AI?
Will it benefit us and our families?
These are probably the two most common questions I get asked in my job.
Well, my answers are: Yes and yes.
This is why we are bringing together countries such as America, France and China today and tomorrow for a summit at Bletchley Park.
They have all been invited as we need all the major players around the table if we are to develop a serious strategy for the decades to come.
In the last couple of years, billions and billions of (mainly) dollars have been invested in making AI more and more powerful, and it’s achieved amazing things.
Like Google DeepMind’s Alphafold which predicts the shape of complex proteins that make up the human body.
Generations of painstaking human scientific breakthroughs have been condensed into moments — opening the door to new medical treatments or even a cure for cancer.
Even the most popular AI models today such as ChatGPT boast astonishing abilities.
A company called Wolfram Alpha recently got it to sit a maths A-Level exam — it scored 96 per cent.
Of course, apps that create photorealistic images from a simple written description can be great fun when producing pictures of cats roller-skating in the park, but can be used to create deep fakes which can ruin lives.
Generative AI can be used to combat air pollution, but could also be deployed to pollute public opinion, and the models designed to discover new drugs could be warped to uncover novel bioweapons.
Just like any new technology, while the vast majority of people are working to ensure it improves our lives, there are those in the world who will inevitably try to use it to do us harm.
That is why I am so committed to ensuring we grip these risks so we can seize the opportunities for our country and our public services, such as the NHS.
The experts agree the country that tackles these risks first will be the first to fully take advantage of the limitless opportunities ahead.
That is why the UK is putting more investment into AI safety right now than any other country in the world.
We do not have decades of time.
AI is improving so quickly that we have to take action now.
And this is why Britain hosting the world’s first global AI Safety Summit is such a historic and important event — looking long-term to build a brighter future.
It marks the beginning of a new international effort to ensure AI benefits us all — giving us faster and cleaner transport, improving education and, most significantly, giving us more time with our loved ones who we would otherwise have had to say goodbye to earlier than we should.
Britain making the long-term decisions to lead the way on this vitally important issue will be something our grandchildren will read about in the history books.
Because being at the centre of safe, responsible AI development means that our values are at the centre too.
Democracy, fairness, freedom, rule of law, tolerance and respect — these are the values that have made our country what it is today, and if we continue leading the world in safe AI then we will ensure these are the values of the future as well.
So, to answer the original questions, it would be madness not to embrace the opportunities presented to us from AI.
Because if we get this right, it will transform all our lives for the better.
It would equally be madness not to take action to manage those risks.
Today the world will come together at Bletchley Park, where Alan Turing pioneered early versions of computing and helped the Allies win World War Two by breaking the German Enigma codes.
I do not underestimate how challenging this issue is.
But ahead of the Bletchley Summit I am guided by Turing’s words that “we can only see a short distance ahead, but we can see plenty there that needs to be done”.
We stand ready to tackle one of the greatest challenges of our time.
GOOGLE Maps users are set to see an AI upgrade that will help them keep their eyes on the road - don't be caught out.
The latest update will offer 3D planned routes, orientation features, predicted weather details and more.
Immersive View for routes provides users with an AI generated preview of their journey.
It even shows predicted weather and traffic details along the planned route which will come in handy while deciding what time to travel.
Google Maps is also introducing features to help users orientate themselves in new locations.
"Simply tap the Lens icon in the search bar and lift your phone to find information about nearby ATMs, transit stations, restaurants, coffee shops and stores," penned Google’s Chris Phillips in a blog post.
And, in the next 12 months an improved navigation tool will be rolled out that helps drivers.
Lane details on the road will become clearer to avoid stressful, last minute changes.
AI is also set to provide motorists with speed limit information to make sure they stay safe on the road when signs are obscured.
Meanwhile, EV owners can wave goodbye to charging anxiety as the nearest stations will be available to see at the tap of a button.
Chris explained: "Starting this week, building on our capabilities in cars with Google built-in, EV drivers on Android and iOS will now see even more helpful charging station information, including whether a charger is compatible with their vehicle and whether the available chargers are fast, medium, or slow to help you find the charging station that best meets your needs."
The upgrade can also help users decide where to go and what to do when they're struggling to find a plan.
They can search for an activity, such as pumpkin patches, and a list of photo-first results will appear to make choosing a location easier and faster.
Users can also type in "things to do" and will be presented with suggestions that suit the area or country they're in.
"Tap on the places that interest you to learn more, and even save the options to a list if you want to check them out later," added Chris.
"Thematic results for activities and dining roll out globally on Android and iOS in the coming weeks."
This comes as an "emoji" update has already taken place on Maps, allowing users to easily identify what a pinned location is.
Meanwhile, a husband ended up divorcing his "cheating" wife after spotting her cuddling her 'lover' on Google Maps.
He saw the Street View evidence of his spouse embracing a stranger while planning his route ahead of a long drive.
Plus, a homeowner who penned an X-rated message on their roof to "mock their nosy neighbours" spying on Maps, didn't expect what happened next.
The property in Lake County, Illinois, US, was spotted by a Facebook user who shared a screenshot of the cheeky giant graffiti.
And, Google has now announced a massive upgrade to its Bard AI chatbot as the firm steps up rivalry against ChatGPT.
The free service will now connect to some of Google's biggest apps, including YouTube, Gmail and Maps.
An enhanced integration means users can ask Bard to get information from their emails and other private documents.
POPULAR artificial intelligence site ChatGPT can teach users how to use drones to drop homemade petrol bombs.
The revelation about the tech comes as PM Rishi Sunak prepares to tackle the threat posed by AI at a world-first global safety summit.
Our investigation proves the chatbot dangers are very real.
We were also able to prompt ChatGPT to generate right-wing terrorist propaganda, phishing scams and computer viruses.
Enlisting the help of a cyber expert, The Sun has exposed just how horrific mainstream AI apps could be in the wrong hands.
Solomon Gilbert, head of cyber at anti-crime company We Fight Fraud, showed our reporter how to obtain details on making a petrol bomb in five short prompts, which will not be revealed for safety reasons.
But it then went on to suggest a drone dropping the petrol bomb would be the “best mode of its delivery”.
We then asked ChatGPT to produce content about far-right terrorist ideology.
In seconds it reeled off hundreds of words of pro-violence material, describing young men yearning to be “part of something larger, something that challenges the status quo”.